#!/usr/bin/env python3
"""
Negi AI Lab - Auto Article Generator
=====================================
AIãƒˆãƒ¬ãƒ³ãƒ‰ç‰¹åŒ–å‹ã®å…¨è‡ªå‹•ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆè¨˜äº‹ç”Ÿæˆã‚¨ãƒ³ã‚¸ãƒ³

ã‚«ãƒ†ã‚´ãƒªãƒ¼æ§‹æˆæ¯”ç‡:
  - NEWS (é€Ÿå ±): 40%
  - TOOL (ãƒ„ãƒ¼ãƒ«æ¤œè¨¼): 40%
  - GUIDE (è§£èª¬/ã‚¬ã‚¤ãƒ‰): 20%

ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½ã«ã‚ˆã‚Šã€ãƒã‚¿ä¸è¶³æ™‚ã¯æ¬¡ã‚«ãƒ†ã‚´ãƒªãƒ¼ã§è£œå¡«ã—ã€
å¿…ãšæŒ‡å®šã•ã‚ŒãŸåˆè¨ˆè¨˜äº‹æ•°ã‚’ç¢ºä¿ã™ã‚‹ã€‚

Usage:
  python auto_generate.py --dry-run          # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ã¿
  python auto_generate.py --test-one         # 1ä»¶ã ã‘ç”Ÿæˆã—ã¦ãƒ†ã‚¹ãƒˆ
  python auto_generate.py --total 10         # 10è¨˜äº‹ç”Ÿæˆ
"""

from __future__ import annotations

import argparse
import io
import json
import os
import random
import re
import time
import uuid
from dataclasses import dataclass, field
from datetime import datetime, timedelta, timezone
from enum import Enum
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from urllib.parse import quote, urlencode

import feedparser
import requests
from bs4 import BeautifulSoup

import google.generativeai as genai

# TwitteræŠ•ç¨¿ç”¨ (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)
try:
    import tweepy
    TWEEPY_AVAILABLE = True
except ImportError:
    TWEEPY_AVAILABLE = False

# Optional: load .env if available
try:
    from dotenv import load_dotenv
    load_dotenv()
except Exception:
    pass


# ============================================================
# Constants & Configuration
# ============================================================

JST = timezone(timedelta(hours=9))
SLEEP_SECONDS_PER_ARTICLE = 30
DEFAULT_TOTAL_ARTICLES = 2  # 1å›ã®å®Ÿè¡Œã§2è¨˜äº‹ç”Ÿæˆ

# ã‚«ãƒ†ã‚´ãƒªãƒ¼ç›®æ¨™æ¯”ç‡ (1æ—¥12è¨˜äº‹: NEWS 5, TOOL 5, GUIDE 2)
CATEGORY_RATIOS = {
    "NEWS": 0.42,   # 5/12 â‰ˆ 0.42
    "TOOL": 0.42,   # 5/12 â‰ˆ 0.42
    "GUIDE": 0.16,  # 2/12 â‰ˆ 0.16
}

# ã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆè¨­å®š
AMAZON_ASSOCIATE_TAG = os.environ.get("AMAZON_ASSOCIATE_TAG", "negi3939-22")
RAKUTEN_AFFILIATE_ID = os.environ.get("RAKUTEN_AFFILIATE_ID", "5000cbfd.5f52567b.5000cbff.924460a4")

# User-Agent for requests
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"


class Category(Enum):
    NEWS = "NEWS"
    TOOL = "TOOL"
    GUIDE = "GUIDE"


@dataclass
class NewsItem:
    """åé›†ã—ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¢ã‚¤ãƒ†ãƒ """
    source: str
    title: str
    url: str
    category: Category
    published: str = ""
    summary: str = ""
    extra: Dict = field(default_factory=dict)


# ============================================================
# Processed URLs Store
# ============================================================

class ProcessedURLStore:
    """é‡è¤‡é˜²æ­¢ç”¨ã®URLæ°¸ç¶šåŒ–ã‚¹ãƒˆã‚¢"""

    def __init__(self, path: Path) -> None:
        self.path = path
        self._urls: List[str] = []
        self._dirty = False

    def load(self) -> List[str]:
        if not self.path.exists():
            self._urls = []
            return self._urls
        try:
            data = json.loads(self.path.read_text(encoding="utf-8"))
            self._urls = [str(x) for x in data] if isinstance(data, list) else []
        except Exception:
            self._urls = []
        return self._urls

    def contains(self, url: str) -> bool:
        if not self._urls:
            self.load()
        return url in self._urls

    def add(self, url: str) -> None:
        if not self._urls:
            self.load()
        if url not in self._urls:
            self._urls.append(url)
            self._dirty = True

    def save(self) -> None:
        if self._dirty:
            self.path.parent.mkdir(parents=True, exist_ok=True)
            self.path.write_text(
                json.dumps(self._urls, ensure_ascii=False, indent=2) + "\n",
                encoding="utf-8",
            )
            self._dirty = False


# ============================================================
# News Collector
# ============================================================

class NewsCollector:
    """å„ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""

    def __init__(self, processed_store: ProcessedURLStore) -> None:
        self.processed_store = processed_store
        self.session = requests.Session()
        self.session.headers.update({"User-Agent": USER_AGENT})

    def _is_fresh(self, url: str) -> bool:
        """URLãŒæœªå‡¦ç†ã‹ã©ã†ã‹"""
        return not self.processed_store.contains(url)

    def _normalize_text(self, text: str) -> str:
        return re.sub(r"\s+", " ", text).strip()

    # -------------------------
    # NEWS Sources
    # -------------------------

    def collect_news(self, max_items: int = 20) -> List[NewsItem]:
        """NEWS ã‚«ãƒ†ã‚´ãƒªãƒ¼: Google News RSS ã‹ã‚‰åé›†"""
        items: List[NewsItem] = []
        items.extend(self._collect_google_news(max_items))
        return items

    def _collect_google_news(self, max_items: int) -> List[NewsItem]:
        query = "Artificial Intelligence OR Gemini OR OpenAI OR Claude"
        rss_url = (
            "https://news.google.com/rss/search?"
            + urlencode({"q": query, "hl": "ja", "gl": "JP", "ceid": "JP:ja"})
        )

        try:
            resp = self.session.get(rss_url, timeout=20)
            resp.raise_for_status()
        except Exception as e:
            print(f"  [!] Google News RSS fetch failed: {e}")
            return []

        feed = feedparser.parse(resp.content)
        entries = getattr(feed, "entries", []) or []

        results: List[NewsItem] = []
        for entry in entries[:max_items * 2]:  # é‡è¤‡è€ƒæ…®ã—ã¦å¤šã‚ã«å–å¾—
            url = (getattr(entry, "link", "") or "").strip()
            title = (getattr(entry, "title", "") or "").strip()
            published = (getattr(entry, "published", "") or "").strip()
            summary = self._normalize_text(getattr(entry, "summary", "") or "")

            if not url or not title:
                continue
            if not self._is_fresh(url):
                continue

            results.append(NewsItem(
                source="Google News",
                title=title,
                url=url,
                category=Category.NEWS,
                published=published,
                summary=summary[:500],
            ))

            if len(results) >= max_items:
                break

        return results

    # -------------------------
    # TOOL Sources
    # -------------------------

    def collect_tools(self, max_items: int = 20) -> List[NewsItem]:
        """TOOL ã‚«ãƒ†ã‚´ãƒªãƒ¼: Product Hunt + GitHub Trending ã‹ã‚‰åé›†"""
        items: List[NewsItem] = []

        # Product Hunt (åŠåˆ†)
        ph_items = self._collect_product_hunt(max_items // 2 + 2)
        items.extend(ph_items)

        # GitHub Trending (æ®‹ã‚Š)
        remaining = max_items - len(items) + 5
        gh_items = self._collect_github_trending(remaining)
        items.extend(gh_items)

        return items[:max_items]

    def _collect_product_hunt(self, max_items: int) -> List[NewsItem]:
        """Product Hunt RSS ã‹ã‚‰ AIé–¢é€£ãƒ„ãƒ¼ãƒ«ã‚’åé›†"""
        rss_url = "https://www.producthunt.com/feed"

        try:
            resp = self.session.get(rss_url, timeout=20)
            resp.raise_for_status()
        except Exception as e:
            print(f"  [!] Product Hunt RSS fetch failed: {e}")
            return []

        feed = feedparser.parse(resp.content)
        entries = getattr(feed, "entries", []) or []

        # AIé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿
        ai_keywords = ["ai", "gpt", "llm", "machine learning", "ml", "neural",
                       "copilot", "chatbot", "automation", "generative"]

        results: List[NewsItem] = []
        for entry in entries:
            url = (getattr(entry, "link", "") or "").strip()
            title = (getattr(entry, "title", "") or "").strip()
            summary = self._normalize_text(getattr(entry, "summary", "") or "")

            if not url or not title:
                continue

            # AIé–¢é€£ãƒã‚§ãƒƒã‚¯
            text_lower = (title + " " + summary).lower()
            if not any(kw in text_lower for kw in ai_keywords):
                continue

            if not self._is_fresh(url):
                continue

            results.append(NewsItem(
                source="Product Hunt",
                title=title,
                url=url,
                category=Category.TOOL,
                summary=summary[:500],
            ))

            if len(results) >= max_items:
                break

        return results

    def _collect_github_trending(self, max_items: int) -> List[NewsItem]:
        """GitHub Trending (machine-learning) ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        url = "https://github.com/trending?since=daily&spoken_language_code=en"

        try:
            resp = self.session.get(url, timeout=20)
            resp.raise_for_status()
        except Exception as e:
            print(f"  [!] GitHub Trending fetch failed: {e}")
            return []

        soup = BeautifulSoup(resp.text, "html.parser")
        repo_list = soup.select("article.Box-row")

        results: List[NewsItem] = []
        for repo in repo_list[:max_items * 2]:
            try:
                h2 = repo.select_one("h2 a")
                if not h2:
                    continue

                repo_path = h2.get("href", "").strip()
                if not repo_path:
                    continue

                repo_url = f"https://github.com{repo_path}"
                repo_name = repo_path.strip("/")

                # Description
                desc_elem = repo.select_one("p")
                description = self._normalize_text(desc_elem.get_text()) if desc_elem else ""

                # Stars today
                stars_elem = repo.select_one("span.d-inline-block.float-sm-right")
                stars_today = self._normalize_text(stars_elem.get_text()) if stars_elem else ""

                if not self._is_fresh(repo_url):
                    continue

                results.append(NewsItem(
                    source="GitHub Trending",
                    title=repo_name,
                    url=repo_url,
                    category=Category.TOOL,
                    summary=description[:500],
                    extra={"stars_today": stars_today},
                ))

                if len(results) >= max_items:
                    break

            except Exception:
                continue

        return results

    # -------------------------
    # GUIDE Sources
    # -------------------------

    def collect_guides(self, max_items: int = 20) -> List[NewsItem]:
        """GUIDE ã‚«ãƒ†ã‚´ãƒªãƒ¼: Reddit + å®šç•ªãƒˆãƒ”ãƒƒã‚¯ã‹ã‚‰åé›†"""
        items: List[NewsItem] = []

        # Reddit
        reddit_items = self._collect_reddit(max_items)
        items.extend(reddit_items)

        # è¶³ã‚Šãªã„å ´åˆã¯å®šç•ªã‚¬ã‚¤ãƒ‰ãƒˆãƒ”ãƒƒã‚¯ã§è£œå¡«
        if len(items) < max_items:
            remaining = max_items - len(items)
            fallback_items = self._generate_guide_topics(remaining)
            items.extend(fallback_items)

        return items[:max_items]

    def _collect_reddit(self, max_items: int) -> List[NewsItem]:
        """Reddit (r/LocalLLaMA, r/OpenAI) ã® Top posts ã‚’åé›†"""
        subreddits = ["LocalLLaMA", "OpenAI", "MachineLearning", "artificial"]
        results: List[NewsItem] = []

        for subreddit in subreddits:
            if len(results) >= max_items:
                break

            # Reddit JSON API (èªè¨¼ä¸è¦)
            url = f"https://www.reddit.com/r/{subreddit}/top.json?t=week&limit=10"

            try:
                resp = self.session.get(url, timeout=20)
                resp.raise_for_status()
                data = resp.json()
            except Exception as e:
                print(f"  [!] Reddit r/{subreddit} fetch failed: {e}")
                continue

            posts = data.get("data", {}).get("children", [])
            for post in posts:
                post_data = post.get("data", {})
                title = post_data.get("title", "").strip()
                permalink = post_data.get("permalink", "")
                selftext = post_data.get("selftext", "")[:500]
                score = post_data.get("score", 0)

                if not title or not permalink:
                    continue

                post_url = f"https://www.reddit.com{permalink}"

                if not self._is_fresh(post_url):
                    continue

                # ã‚¹ã‚³ã‚¢ãŒä½ã™ãã‚‹ã‚‚ã®ã¯é™¤å¤–
                if score < 50:
                    continue

                results.append(NewsItem(
                    source=f"Reddit r/{subreddit}",
                    title=title,
                    url=post_url,
                    category=Category.GUIDE,
                    summary=selftext,
                    extra={"score": score},
                ))

                if len(results) >= max_items:
                    break

        return results

    def _generate_guide_topics(self, count: int) -> List[NewsItem]:
        """å®šç•ªã‚¬ã‚¤ãƒ‰ãƒˆãƒ”ãƒƒã‚¯ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰"""
        topics = [
            ("ãƒ­ãƒ¼ã‚«ãƒ«LLMã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¬ã‚¤ãƒ‰ï¼ˆ2024å¹´ç‰ˆï¼‰", "local-llm-setup"),
            ("Ollama + Open WebUIã§è‡ªå®…AIãƒãƒ£ãƒƒãƒˆã‚’æ§‹ç¯‰ã™ã‚‹æ–¹æ³•", "ollama-webui-setup"),
            ("RAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®åŸºç¤ã¨å®Ÿè£…å…¥é–€", "rag-pipeline-intro"),
            ("LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Œå…¨ã‚¬ã‚¤ãƒ‰", "lora-finetuning-guide"),
            ("LangChainã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä½œã‚‹æ–¹æ³•", "langchain-agent-tutorial"),
            ("Hugging Face Transformersã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ", "hf-transformers-quickstart"),
            ("GPT-4 APIã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹", "gpt4-api-best-practices"),
            ("Stable Diffusion XLãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°", "sdxl-prompt-engineering"),
            ("LLMã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’æœ€å¤§é™æ´»ç”¨ã™ã‚‹ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯", "llm-context-length-tips"),
            ("AIã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆå¾¹åº•æ¯”è¼ƒ", "ai-coding-assistant-comparison"),
            ("Gemini APIå…¥é–€ï¼šæœ€åˆã®ã‚¢ãƒ—ãƒªã‚’ä½œã‚‹", "gemini-api-getting-started"),
            ("Claude APIã®ä½¿ã„æ–¹ã¨Tips", "claude-api-tips"),
            ("ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹é¸å®šã‚¬ã‚¤ãƒ‰", "vector-db-comparison"),
            ("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³å¯¾ç­–å…¥é–€", "prompt-injection-defense"),
            ("ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«AIã®æ´»ç”¨äº‹ä¾‹é›†", "multimodal-ai-use-cases"),
        ]

        random.shuffle(topics)
        results: List[NewsItem] = []

        for title, slug in topics[:count]:
            # ä»®ã®URLã‚’ç”Ÿæˆï¼ˆå®Ÿéš›ã«ã¯URLãªã—ã§è¨˜äº‹ç”Ÿæˆï¼‰
            fake_url = f"guide://{slug}-{uuid.uuid4().hex[:6]}"

            if not self._is_fresh(fake_url):
                continue

            results.append(NewsItem(
                source="Negi Lab Guide",
                title=title,
                url=fake_url,
                category=Category.GUIDE,
                summary="",
                extra={"is_generated_topic": True},
            ))

        return results


# ============================================================
# Article Result Data Class
# ============================================================

@dataclass
class ArticleResult:
    """è¨˜äº‹ç”Ÿæˆçµæœã‚’ä¿æŒã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    title: str
    body: str
    shopping_keyword: Optional[str] = None
    viral_tags: Optional[str] = None  # "#ã‚¿ã‚°1 #ã‚¿ã‚°2" å½¢å¼


# ============================================================
# Article Generator
# ============================================================

class ArticleGenerator:
    """Gemini APIã‚’ä½¿ç”¨ã—ã¦ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¥ã®è¨˜äº‹ã‚’ç”Ÿæˆ"""

    def __init__(self, api_key: str, model_name: str = "gemini-3-flash-preview") -> None:
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(model_name)

    def generate_article(self, item: NewsItem) -> ArticleResult:
        """
        è¨˜äº‹ã‚’ç”Ÿæˆã—ã€ArticleResultã‚’è¿”ã™ã€‚
        """
        prompt = self._build_prompt(item)

        try:
            response = self.model.generate_content(prompt)
        except Exception as e:
            raise RuntimeError(f"Gemini API error: {e}")

        text = getattr(response, "text", None)
        if not text:
            raise RuntimeError("Gemini returned empty response")

        # å¤–å´ã®```markdownãƒ•ã‚§ãƒ³ã‚¹ã®ã¿é™¤å»ï¼ˆå†…éƒ¨ã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã¯ä¿æŒï¼‰
        cleaned = text.strip()
        
        # å…ˆé ­ã®ãƒ•ã‚§ãƒ³ã‚¹é™¤å»
        if cleaned.startswith("```markdown"):
            cleaned = cleaned[len("```markdown"):].strip()
        elif cleaned.startswith("```md"):
            cleaned = cleaned[len("```md"):].strip()
        elif cleaned.startswith("```"):
            cleaned = cleaned[3:].strip()
        
        # æœ«å°¾ã®ãƒ•ã‚§ãƒ³ã‚¹é™¤å»ï¼ˆæœ¬æ–‡æœ«ã®å˜ç‹¬```ã®ã¿å¯¾è±¡ï¼‰
        lines = cleaned.split("\n")
        while lines and lines[-1].strip() == "```":
            lines.pop()
        cleaned = "\n".join(lines).strip()

        return self._extract_title_and_body(cleaned)

    def _build_prompt(self, item: NewsItem) -> str:
        """ã‚«ãƒ†ã‚´ãƒªãƒ¼ã«å¿œã˜ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰"""

        if item.category == Category.NEWS:
            return self._build_news_prompt(item)
        elif item.category == Category.TOOL:
            return self._build_tool_prompt(item)
        else:  # GUIDE
            return self._build_guide_prompt(item)

    def _build_news_prompt(self, item: NewsItem) -> str:
        return f'''ã‚ãªãŸã¯ã€ŒNegi Labã€æ‰€å±ã®è¾›å£ã ãŒæŠ€è¡“ã«è©³ã—ã„AIç ”ç©¶å“¡ã§ã™ã€‚
èª­è€…ã¯ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã‚„ãƒ“ã‚¸ãƒã‚¹ãƒãƒ³ã€‚å®Ÿç”¨æ€§ã¨æŠ€è¡“çš„è¦–ç‚¹ã‚’é‡è¦–ã—ã¦åŸ·ç­†ã—ã¦ãã ã•ã„ã€‚

ã€å…ƒãƒ‹ãƒ¥ãƒ¼ã‚¹æƒ…å ±ã€‘
- ã‚¿ã‚¤ãƒˆãƒ«: {item.title}
- URL: {item.url}
- å‡ºå…¸: {item.source}
{f"- è¦ç´„: {item.summary}" if item.summary else ""}

ã€æŒ‡ç¤ºã€‘
é€Ÿå ±ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚
- ä½•ãŒç™ºè¡¨ã•ã‚ŒãŸã‹
- ç«¶åˆï¼ˆChatGPT, Claudeç­‰ï¼‰ã¨ã®é•ã„ã¯ä½•ã‹
- æ¥­ç•Œã¸ã®å½±éŸ¿ã‚’è«–ç†çš„ã«è§£èª¬

ã€å‡ºåŠ›å½¢å¼ã€‘å¿…ãšä»¥ä¸‹ã®Markdownæ§‹é€ ã§å‡ºåŠ›ã™ã‚‹ã“ã¨ã€‚

1è¡Œç›®: ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆè£…é£¾ãªã—ã€## ãªã©ã¯ä»˜ã‘ãªã„ï¼‰

## 3è¡Œè¦ç´„

- è¦ç´„1
- è¦ç´„2
- è¦ç´„3

## ä½•ãŒç™ºè¡¨ã•ã‚ŒãŸã®ã‹

ï¼ˆè©³ç´°ãªè§£èª¬ï¼‰

## ç«¶åˆã¨ã®æ¯”è¼ƒ

| é …ç›® | ä»Šå›ã®ç™ºè¡¨ | ChatGPT | Claude |
|------|-----------|---------|--------|
| ... | ... | ... | ... |

## æ¥­ç•Œã¸ã®å½±éŸ¿

ï¼ˆè«–ç†çš„ãªåˆ†æï¼‰

## Negi Labã®è¦‹è§£

ï¼ˆè¾›å£ã ãŒå»ºè¨­çš„ãªã‚³ãƒ¡ãƒ³ãƒˆï¼‰

---
### ã€é‡è¦ã€‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é¸å®šã‚¿ã‚¹ã‚¯
è¨˜äº‹åŸ·ç­†å¾Œã€ä»¥ä¸‹ã®2ç¨®é¡ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚»ãƒƒãƒˆã‚’é¸å®šã—ã€è¨˜äº‹ã®æœ«å°¾ã«æŒ‡å®šã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§è¿½è¨˜ã—ã¦ãã ã•ã„ã€‚

**1. å•†å“æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ (Shopping Keyword)**
èª­è€…ãŒAmazonã‚„æ¥½å¤©ã§æ¤œç´¢ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ãŸéš›ã€æœ€ã‚‚é©åˆ‡ãªå•†å“ä¸€è¦§ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã‚ˆã†ãªã€Œå…·ä½“çš„ãªè£½å“åã€ã‚„ã€Œå‹ç•ªã€ã‚’é¸å®šã™ã‚‹ã“ã¨ã€‚
- **ç¦æ­¢:** ã€ŒPCã€ã€ŒGPUã€ã®ã‚ˆã†ãªåºƒã™ãã‚‹1å˜èªï¼ˆãƒã‚¤ã‚ºå•†å“ãŒæ··ã–ã‚‹ï¼‰
- **ç¦æ­¢:** é•·ã™ãã‚‹æ­£å¼åç§°ï¼ˆæ¤œç´¢ãƒ’ãƒƒãƒˆ0ã«ãªã‚‹ï¼‰
- **æ¨å¥¨:** **2ã€œ3å˜èª**ã®çµ„ã¿åˆã‚ã›ï¼ˆä¾‹: "MacBook Air M3", "RTX 4070 Ti", "Python ã‚ªãƒ©ã‚¤ãƒªãƒ¼"ï¼‰
- ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: `[SHOPPING: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰]`

**2. SNSæ‹¡æ•£ç”¨ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚° (Viral Tags)**
X (Twitter) ã§ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã‚’ç¨¼ããŸã‚ã®ã€éœ€è¦ãŒã‚ã‚Šè¨˜äº‹å†…å®¹ã«é–¢é€£ã™ã‚‹ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ¯ãƒ¼ãƒ‰ã€‚
- **æ—¥æœ¬èªã§ã€ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ï¼ˆ#ï¼‰ä»˜ãã§2ã¤ã ã‘**é¸å®š
- å•†å“åã§ã¯ãªãã€Œèˆˆå‘³é–¢å¿ƒè»¸ã€ã§é¸ã¶ï¼ˆä¾‹: RTX4090ã®è¨˜äº‹ãªã‚‰ `#è‡ªä½œPC` `#ã‚²ãƒ¼ãƒŸãƒ³ã‚°PC`ï¼‰
- ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: `[HASHTAGS: #ã‚¿ã‚°1 #ã‚¿ã‚°2]`
---

ã€ç¦æ­¢äº‹é …ã€‘
- YAML/TOML Front Matterã¯å‡ºåŠ›ã—ãªã„
- HTMLã‚¿ã‚°ã¯ä½¿ã‚ãªã„
'''

    def _build_tool_prompt(self, item: NewsItem) -> str:
        stars_info = f"- ä»Šæ—¥ã®ã‚¹ã‚¿ãƒ¼æ•°: {item.extra['stars_today']}" if item.extra.get("stars_today") else ""

        return f'''ã‚ãªãŸã¯ã€ŒNegi Labã€æ‰€å±ã®è¾›å£ã ãŒæŠ€è¡“ã«è©³ã—ã„AIç ”ç©¶å“¡ã§ã™ã€‚
èª­è€…ã¯ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã‚„ãƒ“ã‚¸ãƒã‚¹ãƒãƒ³ã€‚å®Ÿç”¨æ€§ã¨æŠ€è¡“çš„è¦–ç‚¹ã‚’é‡è¦–ã—ã¦åŸ·ç­†ã—ã¦ãã ã•ã„ã€‚

ã€å¯¾è±¡ãƒ„ãƒ¼ãƒ«æƒ…å ±ã€‘
- åå‰: {item.title}
- URL: {item.url}
- å‡ºå…¸: {item.source}
{f"- èª¬æ˜: {item.summary}" if item.summary else ""}
{stars_info}

ã€æŒ‡ç¤ºã€‘
ã€Œ**æ¤œè¨¼ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³**ã€ã‚’å«ã‚€ãƒ„ãƒ¼ãƒ«ãƒ¬ãƒ“ãƒ¥ãƒ¼è¨˜äº‹ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚
å®Ÿéš›ã«ãã®ãƒ„ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦å‹•ã‹ã—ãŸã¨ä»®å®šã—ã€ä»¥ä¸‹ã‚’å«ã‚ã‚‹ã“ã¨ï¼š
- å…·ä½“çš„ãªPythonã‚³ãƒ¼ãƒ‰ä¾‹ï¼ˆå‹•ä½œã™ã‚‹é¢¨ã®ã‚³ãƒ¼ãƒ‰ï¼‰
- å®Ÿè¡Œçµæœã®ä¾‹ï¼ˆæ¶ç©ºã§OKï¼‰
- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¾‹ï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰

â€»è¨˜äº‹å†’é ­ã«ã€Œã“ã‚Œã¯ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã‚ã‚Šå®Ÿæ¸¬ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€ã¨æ˜è¨˜ã™ã‚‹ã“ã¨ã€‚

ã€å‡ºåŠ›å½¢å¼ã€‘å¿…ãšä»¥ä¸‹ã®Markdownæ§‹é€ ã§å‡ºåŠ›ã™ã‚‹ã“ã¨ã€‚

1è¡Œç›®: ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆè£…é£¾ãªã—ã€ã‚­ãƒ£ãƒƒãƒãƒ¼ã«ï¼‰

> âš ï¸ æœ¬è¨˜äº‹ã®æ¤œè¨¼ãƒ‘ãƒ¼ãƒˆã¯ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã‚ã‚Šã€å®Ÿéš›ã®æ¸¬å®šçµæœã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

## 3è¡Œè¦ç´„

- è¦ç´„1
- è¦ç´„2
- è¦ç´„3

## ã“ã®ãƒ„ãƒ¼ãƒ«ã¯ä½•ã‹

ï¼ˆæ¦‚è¦èª¬æ˜ï¼‰

## æ¤œè¨¼ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼šå®Ÿéš›ã«ä½¿ã£ã¦ã¿ãŸ

### ç’°å¢ƒæ§‹ç¯‰

```bash
pip install xxx
```

### åŸºæœ¬çš„ãªä½¿ã„æ–¹

```python
# ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ï¼ˆæ¶ç©ºã§OKï¼‰
from xxx import YYY

model = YYY()
result = model.run("ãƒ†ã‚¹ãƒˆå…¥åŠ›")
print(result)
```

### å®Ÿè¡Œçµæœ

```
ï¼ˆæ¶ç©ºã®å‡ºåŠ›ä¾‹ï¼‰
```

## ãƒ¡ãƒªãƒƒãƒˆãƒ»ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ

### ãƒ¡ãƒªãƒƒãƒˆ
- ...

### ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ
- ...

## çµè«–ï¼šNegi Labã®è©•ä¾¡

ï¼ˆæ˜Ÿè©•ä¾¡: â˜…â˜…â˜…â˜†â˜† ã®ã‚ˆã†ãªå½¢å¼ã‚‚å¯ï¼‰

---
### ã€é‡è¦ã€‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é¸å®šã‚¿ã‚¹ã‚¯
è¨˜äº‹åŸ·ç­†å¾Œã€ä»¥ä¸‹ã®2ç¨®é¡ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚»ãƒƒãƒˆã‚’é¸å®šã—ã€è¨˜äº‹ã®æœ«å°¾ã«æŒ‡å®šã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§è¿½è¨˜ã—ã¦ãã ã•ã„ã€‚

**1. å•†å“æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ (Shopping Keyword)**
èª­è€…ãŒAmazonã‚„æ¥½å¤©ã§æ¤œç´¢ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ãŸéš›ã€æœ€ã‚‚é©åˆ‡ãªå•†å“ä¸€è¦§ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã‚ˆã†ãªã€Œå…·ä½“çš„ãªè£½å“åã€ã‚„ã€Œå‹ç•ªã€ã‚’é¸å®šã™ã‚‹ã“ã¨ã€‚
- **ç¦æ­¢:** ã€ŒPCã€ã€ŒGPUã€ã®ã‚ˆã†ãªåºƒã™ãã‚‹1å˜èªï¼ˆãƒã‚¤ã‚ºå•†å“ãŒæ··ã–ã‚‹ï¼‰
- **ç¦æ­¢:** é•·ã™ãã‚‹æ­£å¼åç§°ï¼ˆæ¤œç´¢ãƒ’ãƒƒãƒˆ0ã«ãªã‚‹ï¼‰
- **æ¨å¥¨:** **2ã€œ3å˜èª**ã®çµ„ã¿åˆã‚ã›ï¼ˆä¾‹: "RTX 4090", "Raspberry Pi 5", "Python å…¥é–€æ›¸"ï¼‰
- ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: `[SHOPPING: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰]`

**2. SNSæ‹¡æ•£ç”¨ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚° (Viral Tags)**
X (Twitter) ã§ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã‚’ç¨¼ããŸã‚ã®ã€éœ€è¦ãŒã‚ã‚Šè¨˜äº‹å†…å®¹ã«é–¢é€£ã™ã‚‹ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ¯ãƒ¼ãƒ‰ã€‚
- **æ—¥æœ¬èªã§ã€ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ï¼ˆ#ï¼‰ä»˜ãã§2ã¤ã ã‘**é¸å®š
- å•†å“åã§ã¯ãªãã€Œèˆˆå‘³é–¢å¿ƒè»¸ã€ã§é¸ã¶ï¼ˆä¾‹: é–‹ç™ºãƒ„ãƒ¼ãƒ«ã®è¨˜äº‹ãªã‚‰ `#ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°` `#ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢`ï¼‰
- ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: `[HASHTAGS: #ã‚¿ã‚°1 #ã‚¿ã‚°2]`
---

ã€ç¦æ­¢äº‹é …ã€‘
- YAML/TOML Front Matterã¯å‡ºåŠ›ã—ãªã„
- HTMLã‚¿ã‚°ã¯ä½¿ã‚ãªã„
'''

    def _build_guide_prompt(self, item: NewsItem) -> str:
        is_generated = item.extra.get("is_generated_topic", False)
        topic_info = f"- ãƒˆãƒ”ãƒƒã‚¯: {item.title}" if is_generated else f'''- ã‚¿ã‚¤ãƒˆãƒ«: {item.title}
- URL: {item.url}
- å‡ºå…¸: {item.source}
{f"- å†…å®¹: {item.summary}" if item.summary else ""}'''

        return f'''ã‚ãªãŸã¯ã€ŒNegi Labã€æ‰€å±ã®è¾›å£ã ãŒæŠ€è¡“ã«è©³ã—ã„AIç ”ç©¶å“¡ã§ã™ã€‚
èª­è€…ã¯ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã‚„ãƒ“ã‚¸ãƒã‚¹ãƒãƒ³ã€‚å®Ÿç”¨æ€§ã¨æŠ€è¡“çš„è¦–ç‚¹ã‚’é‡è¦–ã—ã¦åŸ·ç­†ã—ã¦ãã ã•ã„ã€‚

ã€ãƒˆãƒ”ãƒƒã‚¯æƒ…å ±ã€‘
{topic_info}

ã€æŒ‡ç¤ºã€‘
ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«å½¢å¼ã®ã‚¬ã‚¤ãƒ‰è¨˜äº‹ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚
ã€Œã€œã™ã‚‹æ–¹æ³•ã€ã€Œã‚¨ãƒ©ãƒ¼å›é¿æ‰‹é †ã€ãªã©ã€èª­è€…ãŒæ‰‹å…ƒã§è©¦ã›ã‚‹ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ã®æ‰‹é †æ›¸ã¨ã—ã¦æ›¸ãã“ã¨ã€‚
- å…·ä½“çš„ãªã‚³ãƒãƒ³ãƒ‰ä¾‹
- ã‚³ãƒ¼ãƒ‰ä¾‹
- è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ä¾‹
ã‚’å¿…ãšå«ã‚ã‚‹ã“ã¨ã€‚

ã€å‡ºåŠ›å½¢å¼ã€‘å¿…ãšä»¥ä¸‹ã®Markdownæ§‹é€ ã§å‡ºåŠ›ã™ã‚‹ã“ã¨ã€‚

1è¡Œç›®: ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆè£…é£¾ãªã—ã€ã€Œã€œã™ã‚‹æ–¹æ³•ã€ã€Œã€œå…¥é–€ã€å½¢å¼æ¨å¥¨ï¼‰

## ã“ã®è¨˜äº‹ã§å­¦ã¹ã‚‹ã“ã¨

- ãƒã‚¤ãƒ³ãƒˆ1
- ãƒã‚¤ãƒ³ãƒˆ2
- ãƒã‚¤ãƒ³ãƒˆ3

## å‰ææ¡ä»¶

- å¿…è¦ãªã‚‚ã®1
- å¿…è¦ãªã‚‚ã®2

## Step 1: ç’°å¢ƒæº–å‚™

```bash
# ã‚³ãƒãƒ³ãƒ‰ä¾‹
```

## Step 2: åŸºæœ¬è¨­å®š

```python
# è¨­å®šã‚³ãƒ¼ãƒ‰ä¾‹
```

## Step 3: å®Ÿè¡Œã¨ç¢ºèª

ï¼ˆæ‰‹é †ã®èª¬æ˜ï¼‰

## ã‚ˆãã‚ã‚‹ã‚¨ãƒ©ãƒ¼ã¨å¯¾å‡¦æ³•

### ã‚¨ãƒ©ãƒ¼1: xxx

```
ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä¾‹
```

**è§£æ±ºç­–:** ...

## ã¾ã¨ã‚ã¨æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

ï¼ˆç· ã‚ã®è¨€è‘‰ã¨ã€æ¬¡ã«å­¦ã¶ã¹ãã“ã¨ã¸ã®èª˜å°ï¼‰

---
### ã€é‡è¦ã€‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é¸å®šã‚¿ã‚¹ã‚¯
è¨˜äº‹åŸ·ç­†å¾Œã€ä»¥ä¸‹ã®2ç¨®é¡ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚»ãƒƒãƒˆã‚’é¸å®šã—ã€è¨˜äº‹ã®æœ«å°¾ã«æŒ‡å®šã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§è¿½è¨˜ã—ã¦ãã ã•ã„ã€‚

**1. å•†å“æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ (Shopping Keyword)**
èª­è€…ãŒAmazonã‚„æ¥½å¤©ã§æ¤œç´¢ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ãŸéš›ã€æœ€ã‚‚é©åˆ‡ãªå•†å“ä¸€è¦§ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã‚ˆã†ãªã€Œå…·ä½“çš„ãªè£½å“åã€ã‚„ã€Œå‹ç•ªã€ã‚’é¸å®šã™ã‚‹ã“ã¨ã€‚
- **ç¦æ­¢:** ã€ŒPCã€ã€ŒGPUã€ã®ã‚ˆã†ãªåºƒã™ãã‚‹1å˜èªï¼ˆãƒã‚¤ã‚ºå•†å“ãŒæ··ã–ã‚‹ï¼‰
- **ç¦æ­¢:** é•·ã™ãã‚‹æ­£å¼åç§°ï¼ˆæ¤œç´¢ãƒ’ãƒƒãƒˆ0ã«ãªã‚‹ï¼‰
- **æ¨å¥¨:** **2ã€œ3å˜èª**ã®çµ„ã¿åˆã‚ã›ï¼ˆä¾‹: "NVIDIA Jetson Nano", "Arduino ã‚¹ã‚¿ãƒ¼ã‚¿ãƒ¼ã‚­ãƒƒãƒˆ", "æ©Ÿæ¢°å­¦ç¿’ å…¥é–€æ›¸"ï¼‰
- ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: `[SHOPPING: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰]`

**2. SNSæ‹¡æ•£ç”¨ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚° (Viral Tags)**
X (Twitter) ã§ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã‚’ç¨¼ããŸã‚ã®ã€éœ€è¦ãŒã‚ã‚Šè¨˜äº‹å†…å®¹ã«é–¢é€£ã™ã‚‹ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ¯ãƒ¼ãƒ‰ã€‚
- **æ—¥æœ¬èªã§ã€ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ï¼ˆ#ï¼‰ä»˜ãã§2ã¤ã ã‘**é¸å®š
- å•†å“åã§ã¯ãªãã€Œèˆˆå‘³é–¢å¿ƒè»¸ã€ã§é¸ã¶ï¼ˆä¾‹: ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«è¨˜äº‹ãªã‚‰ `#ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°åˆå¿ƒè€…` `#ç‹¬å­¦`ï¼‰
- ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: `[HASHTAGS: #ã‚¿ã‚°1 #ã‚¿ã‚°2]`
---

ã€ç¦æ­¢äº‹é …ã€‘
- YAML/TOML Front Matterã¯å‡ºåŠ›ã—ãªã„
- HTMLã‚¿ã‚°ã¯ä½¿ã‚ãªã„
'''

    def _extract_title_and_body(self, text: str) -> ArticleResult:
        """
        Geminiå‡ºåŠ›ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã€æœ¬æ–‡ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã‚’æŠ½å‡ºã€‚
        """
        lines = [ln.rstrip() for ln in text.splitlines()]
        title = ""
        body_start = 0

        for i, ln in enumerate(lines):
            stripped = ln.strip()
            if stripped:
                # ã‚¿ã‚¤ãƒˆãƒ«è¡Œã‚’æŠ½å‡º
                title = stripped.strip('"').strip("'")
                title = re.sub(r"^#{1,6}\s+", "", title).strip()
                body_start = i + 1
                break

        if not title:
            title = "AIãƒˆãƒ¬ãƒ³ãƒ‰è¨˜äº‹ï¼ˆè‡ªå‹•ç”Ÿæˆï¼‰"

        body = "\n".join(lines[body_start:]).lstrip("\n")
        if not body:
            body = "(æœ¬æ–‡ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ)"

        # ãƒ€ãƒ–ãƒ«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã¨ã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆãƒªãƒ³ã‚¯è¿½åŠ 
        body, shopping_keyword, viral_tags = self._extract_keywords_and_add_affiliate(body)

        return ArticleResult(
            title=title,
            body=body,
            shopping_keyword=shopping_keyword,
            viral_tags=viral_tags,
        )

    def _extract_keywords_and_add_affiliate(self, body: str) -> Tuple[str, Optional[str], Optional[str]]:
        """
        æœ¬æ–‡ã‹ã‚‰[SHOPPING: xxx]ã¨[HASHTAGS: xxx]ã‚’æŠ½å‡ºã—ã€ã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆãƒªãƒ³ã‚¯ã‚’è¿½åŠ ã™ã‚‹ã€‚
        
        Returns:
            (å‡¦ç†æ¸ˆã¿æœ¬æ–‡, shopping_keyword, viral_tags)
        """
        # 1. Shopping Keyword æŠ½å‡º
        shopping_pattern = r'\[SHOPPING:\s*(.+?)\]'
        shopping_match = re.search(shopping_pattern, body)
        shopping_keyword = None
        if shopping_match:
            shopping_keyword = shopping_match.group(1).strip()
            # è¡Œå…¨ä½“ã‚’å‰Šé™¤ï¼ˆå‰å¾Œã®ç©ºè¡Œã‚‚å«ã‚€ï¼‰
            body = re.sub(r'\n*\[SHOPPING:[^\]]+\]\n*', '\n', body)
        
        # 2. Viral Tags æŠ½å‡º
        hashtags_pattern = r'\[HASHTAGS:\s*(.+?)\]'
        hashtags_match = re.search(hashtags_pattern, body)
        viral_tags = None
        if hashtags_match:
            viral_tags = hashtags_match.group(1).strip()
            # è¡Œå…¨ä½“ã‚’å‰Šé™¤ï¼ˆå‰å¾Œã®ç©ºè¡Œã‚‚å«ã‚€ï¼‰
            body = re.sub(r'\n*\[HASHTAGS:[^\]]+\]\n*', '\n', body)
        
        # 3. æ—§å½¢å¼ã®[KEYWORD: xxx]ã‚‚å¿µã®ãŸã‚é™¤å»ï¼ˆäº’æ›æ€§ï¼‰
        body = re.sub(r'\n*\[KEYWORD:[^\]]+\]\n*', '\n', body)
        
        # 4. æœ«å°¾ã®ã€Œ---ã€ä»¥é™ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é¸å®šã‚¿ã‚¹ã‚¯æŒ‡ç¤ºã‚‚é™¤å»
        body = re.sub(r'\n---\n### ã€é‡è¦ã€‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é¸å®šã‚¿ã‚¹ã‚¯[\s\S]*$', '', body)
        
        # 5. ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆé€£ç¶šã™ã‚‹ç©ºè¡Œã‚’æ•´ç†ï¼‰
        body = re.sub(r'\n{3,}', '\n\n', body).strip()
        
        # 6. ã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆãƒªãƒ³ã‚¯ã‚’è¿½åŠ 
        if shopping_keyword:
            affiliate_section = self._generate_affiliate_links(shopping_keyword)
            body = body + "\n\n" + affiliate_section
        
        return body, shopping_keyword, viral_tags

    def _generate_affiliate_links(self, keyword: str) -> str:
        """
        ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰Amazon/æ¥½å¤©ã®ã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆãƒªãƒ³ã‚¯ãƒœã‚¿ãƒ³ã‚’ç”Ÿæˆã€‚
        """
        encoded_keyword = quote(keyword, safe="")
        
        amazon_url = f"https://www.amazon.co.jp/s?k={encoded_keyword}&tag={AMAZON_ASSOCIATE_TAG}"
        rakuten_url = f"https://search.rakuten.co.jp/search/mall/{encoded_keyword}/?scid={RAKUTEN_AFFILIATE_ID}"
        
        # Markdown + ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³CSS ã§ãƒœã‚¿ãƒ³é¢¨ãƒªãƒ³ã‚¯ã‚’ç”Ÿæˆ
        affiliate_html = f'''
---

## é–¢é€£å•†å“ã‚’ãƒã‚§ãƒƒã‚¯

<div style="display: flex; gap: 12px; flex-wrap: wrap; margin: 20px 0;">
  <a href="{amazon_url}" target="_blank" rel="noopener noreferrer sponsored" style="display: inline-flex; align-items: center; gap: 8px; padding: 12px 24px; background: linear-gradient(135deg, #ff9900 0%, #ff6600 100%); color: white; text-decoration: none; border-radius: 8px; font-weight: bold; box-shadow: 0 4px 12px rgba(255, 153, 0, 0.3); transition: transform 0.2s;">
    <svg width="20" height="20" viewBox="0 0 24 24" fill="currentColor"><path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm-2 15l-5-5 1.41-1.41L10 14.17l7.59-7.59L19 8l-9 9z"/></svg>
    Amazonã§ã€Œ{keyword}ã€ã‚’æ¤œç´¢
  </a>
  <a href="{rakuten_url}" target="_blank" rel="noopener noreferrer sponsored" style="display: inline-flex; align-items: center; gap: 8px; padding: 12px 24px; background: linear-gradient(135deg, #bf0000 0%, #8b0000 100%); color: white; text-decoration: none; border-radius: 8px; font-weight: bold; box-shadow: 0 4px 12px rgba(191, 0, 0, 0.3); transition: transform 0.2s;">
    <svg width="20" height="20" viewBox="0 0 24 24" fill="currentColor"><path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm-2 15l-5-5 1.41-1.41L10 14.17l7.59-7.59L19 8l-9 9z"/></svg>
    æ¥½å¤©ã§ã€Œ{keyword}ã€ã‚’æ¤œç´¢
  </a>
</div>

<small style="color: #888;">â€»ä¸Šè¨˜ãƒªãƒ³ã‚¯ã¯ã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆãƒªãƒ³ã‚¯ã§ã™ã€‚è³¼å…¥ã«ã‚ˆã‚Šå½“ã‚µã‚¤ãƒˆã«åç›ŠãŒç™ºç”Ÿã™ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚</small>
'''
        return affiliate_html


# ============================================================
# Image Handler
# ============================================================

class ImageHandler:
    """Pollinations.ai ã‚’ä½¿ç”¨ã—ãŸç”»åƒç”Ÿæˆãƒ»ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜"""

    def __init__(self, api_key: str, model_name: str = "gemini-3-flash-preview") -> None:
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(model_name)

    def generate_and_save_image(
        self,
        title: str,
        body: str,
        category: Category,
        article_id: str,
        output_dir: Path,
    ) -> str:
        """
        è¨˜äº‹å†…å®¹ã«åŸºã¥ã„ãŸç”»åƒã‚’ç”Ÿæˆã—ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜ã€‚
        
        Args:
            title: è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«
            body: è¨˜äº‹æœ¬æ–‡
            category: ã‚«ãƒ†ã‚´ãƒªãƒ¼
            article_id: è¨˜äº‹IDï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åç”¨ï¼‰
            output_dir: ç”»åƒä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (static/images/posts/)
        
        Returns:
            Hugoç”¨ã®ç›¸å¯¾ãƒ‘ã‚¹ (ä¾‹: /images/posts/2026-01-13-abc123.png)
        """
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
        prompt_en = self._generate_image_prompt(title, body, category)
        
        # Pollinations.aiã‹ã‚‰ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        encoded = quote(prompt_en, safe="")
        image_url = f"https://image.pollinations.ai/prompt/{encoded}?width=1200&height=630&nologo=true"
        
        try:
            response = requests.get(image_url, timeout=60)
            response.raise_for_status()
            
            # ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            filename = f"{article_id}.png"
            file_path = output_dir / filename
            file_path.write_bytes(response.content)
            
            # Hugoç”¨ç›¸å¯¾ãƒ‘ã‚¹
            return f"/images/posts/{filename}"
            
        except Exception as e:
            print(f"  [Image] Download failed: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: URLã‚’è¿”ã™ï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™æ™‚ç”¨ï¼‰
            return image_url

    def download_image_to_bytes(self, image_path_or_url: str, static_dir: Path) -> Optional[bytes]:
        """
        ç”»åƒã‚’ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦å–å¾—ï¼ˆTwitteræŠ•ç¨¿ç”¨ï¼‰ã€‚
        
        Args:
            image_path_or_url: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ï¼ˆ/images/posts/xxx.pngï¼‰ã¾ãŸã¯URL
            static_dir: staticãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
        
        Returns:
            ç”»åƒã®ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿
        """
        try:
            if image_path_or_url.startswith("http"):
                # URLã®å ´åˆã¯ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                response = requests.get(image_path_or_url, timeout=30)
                response.raise_for_status()
                return response.content
            else:
                # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã®å ´åˆã¯ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
                # /images/posts/xxx.png -> static/images/posts/xxx.png
                local_path = static_dir / image_path_or_url.lstrip("/")
                if local_path.exists():
                    return local_path.read_bytes()
                return None
        except Exception as e:
            print(f"  [Image] Failed to load: {e}")
            return None

    def _generate_image_prompt(self, title: str, body: str, category: Category) -> str:
        style_hint = {
            Category.NEWS: "news broadcast, breaking news style, professional, no text",
            Category.TOOL: "software interface, tech product, modern UI, no text",
            Category.GUIDE: "tutorial, educational diagram, clean design, no text",
        }.get(category, "technology")

        prompt = (
            "Create ONE English image generation prompt for a blog thumbnail.\n"
            "Rules:\n"
            "- Output ONE line only, no quotes\n"
            "- English only\n"
            "- MUST NOT include any text, logos, watermarks, QR codes, or UI elements\n"
            "- Focus on abstract visuals, technology imagery, or symbolic representations\n"
            f"- Style hint: {style_hint}\n"
            "- Make it modern, clean, professional\n\n"
            f"Blog title: {title}\n"
            f"Content preview: {body[:500]}\n"
        )

        try:
            response = self.model.generate_content(prompt)
            text = getattr(response, "text", None)
            if text:
                one_line = text.strip().splitlines()[0].strip().strip('"').strip("'")
                if one_line:
                    return one_line
        except Exception:
            pass

        # Fallback
        return f"Abstract futuristic technology visualization, {style_hint}, minimalist, high quality, 4k, no text"


# ============================================================
# Twitter (X) Poster
# ============================================================

class TwitterPoster:
    """X (Twitter) ã¸ã®è‡ªå‹•æŠ•ç¨¿ã‚’è¡Œã†ã‚¯ãƒ©ã‚¹ï¼ˆç„¡æ–™ãƒ—ãƒ©ãƒ³å¯¾å¿œï¼‰"""

    def __init__(self) -> None:
        """
        ç’°å¢ƒå¤‰æ•°ã‹ã‚‰èªè¨¼æƒ…å ±ã‚’èª­ã¿è¾¼ã¿ã€Tweepy Clientã‚’åˆæœŸåŒ–ã€‚
        
        å¿…è¦ãªç’°å¢ƒå¤‰æ•° (OAuth 1.0a):
          - TWITTER_API_KEY (Consumer Key)
          - TWITTER_API_SECRET (Consumer Secret)
          - TWITTER_ACCESS_TOKEN
          - TWITTER_ACCESS_TOKEN_SECRET
        
        ã‚ªãƒ—ã‚·ãƒ§ãƒ³ (OAuth 2.0 - å°†æ¥ã®æ‹¡å¼µç”¨):
          - TWITTER_CLIENT_ID
          - TWITTER_CLIENT_SECRET
        """
        if not TWEEPY_AVAILABLE:
            raise RuntimeError("tweepy is not installed. Run: pip install tweepy")

        self.api_key = os.getenv("TWITTER_API_KEY", "")
        self.api_secret = os.getenv("TWITTER_API_SECRET", "")
        self.access_token = os.getenv("TWITTER_ACCESS_TOKEN", "")
        self.access_token_secret = os.getenv("TWITTER_ACCESS_TOKEN_SECRET", "")

        if not all([self.api_key, self.api_secret, self.access_token, self.access_token_secret]):
            raise RuntimeError("Twitter API credentials not configured in environment variables")

        # v1.1 API (for media upload - requires Basic plan $100/month)
        auth = tweepy.OAuth1UserHandler(
            self.api_key,
            self.api_secret,
            self.access_token,
            self.access_token_secret,
        )
        self.api_v1 = tweepy.API(auth)

        # v2 API (for tweeting - works with Free plan)
        self.client = tweepy.Client(
            consumer_key=self.api_key,
            consumer_secret=self.api_secret,
            access_token=self.access_token,
            access_token_secret=self.access_token_secret,
        )

    def post_article(
        self,
        title: str,
        url: str,
        category: Category,
        viral_tags: Optional[str] = None,
    ) -> bool:
        """
        è¨˜äº‹ã‚’Twitterã«æŠ•ç¨¿ã™ã‚‹ï¼ˆURLã®ã¿ã€Twitterã‚«ãƒ¼ãƒ‰ã§ç”»åƒè¡¨ç¤ºï¼‰ã€‚

        Args:
            title: è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«
            url: è¨˜äº‹ã®URL (Twitterã‚«ãƒ¼ãƒ‰ã§è‡ªå‹•çš„ã«OGPç”»åƒãŒè¡¨ç¤ºã•ã‚Œã‚‹)
            category: è¨˜äº‹ã‚«ãƒ†ã‚´ãƒªãƒ¼
            viral_tags: GeminiãŒé¸ã‚“ã SNSæ‹¡æ•£ç”¨ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ï¼ˆä¾‹: "#è‡ªä½œPC #ã‚²ãƒ¼ãƒŸãƒ³ã‚°"ï¼‰

        Returns:
            æŠ•ç¨¿æˆåŠŸæ™‚True
        """
        try:
            # ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¥ã‚¢ã‚¤ã‚³ãƒ³
            category_icons = {
                Category.NEWS: "ğŸ“°",
                Category.TOOL: "ğŸ› ï¸",
                Category.GUIDE: "ğŸ“–",
            }
            icon = category_icons.get(category, "ğŸ“¢")

            # ã‚¿ã‚¤ãƒˆãƒ«ã‚’çŸ­ç¸®ï¼ˆ80æ–‡å­—åˆ¶é™ï¼‰
            max_title_len = 80
            short_title = title[:max_title_len] + "..." if len(title) > max_title_len else title

            # ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°æ§‹æˆ
            # viral_tags ãŒ "#ã‚¿ã‚°1 #ã‚¿ã‚°2" å½¢å¼ã§æ¥ã‚‹æƒ³å®š
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: viral_tags ãŒãªã„å ´åˆã¯ã‚«ãƒ†ã‚´ãƒªã«å¿œã˜ãŸãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            if viral_tags:
                # GeminiãŒé¸ã‚“ã ã‚¿ã‚°ã‚’ä½¿ç”¨
                tag_str = viral_tags
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¿ã‚°
                default_tags = {
                    Category.NEWS: "#AIé€Ÿå ± #ãƒ†ãƒƒã‚¯",
                    Category.TOOL: "#é–‹ç™ºãƒ„ãƒ¼ãƒ« #ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢",
                    Category.GUIDE: "#ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚° #å­¦ç¿’",
                }
                tag_str = default_tags.get(category, "#AI #ãƒ†ãƒƒã‚¯")

            # ãƒ„ã‚¤ãƒ¼ãƒˆæœ¬æ–‡ã‚’æ§‹æˆ
            # {icon} {ã‚¿ã‚¤ãƒˆãƒ«}
            # è©³ç´°ã¯ã“ã¡ã‚‰ğŸ‘‡
            # {URL}
            # {viral_tags} #NegiLab
            tweet_text = f"{icon} {short_title}\n\nè©³ç´°ã¯ã“ã¡ã‚‰ğŸ‘‡\n{url}\n\n{tag_str} #NegiLab"

            # ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã§æŠ•ç¨¿ï¼ˆURLã‹ã‚‰Twitterã‚«ãƒ¼ãƒ‰ãŒç”Ÿæˆã•ã‚Œã‚‹ï¼‰
            self.client.create_tweet(text=tweet_text)

            return True

        except Exception as e:
            print(f"  [Twitter] Error: {e}")
            return False


def is_twitter_configured() -> bool:
    """Twitter APIèªè¨¼æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª"""
    required = [
        "TWITTER_API_KEY",
        "TWITTER_API_SECRET",
        "TWITTER_ACCESS_TOKEN",
        "TWITTER_ACCESS_TOKEN_SECRET",
    ]
    return all(os.getenv(key) for key in required)


# ============================================================
# Hugo Markdown Writer
# ============================================================

def write_hugo_markdown(
    out_path: Path,
    title: str,
    date_jst: datetime,
    image_url: str,
    category: Category,
    tags: List[str],
    body: str,
) -> None:
    """Hugoå½¢å¼ã®Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡ºåŠ›ï¼ˆPaperModãƒ†ãƒ¼ãƒå¯¾å¿œï¼‰"""
    out_path.parent.mkdir(parents=True, exist_ok=True)

    # ã‚«ãƒ†ã‚´ãƒªãƒ¼åã‚’Hugoç”¨ã«å¤‰æ›
    category_map = {
        Category.NEWS: "AI News",
        Category.TOOL: "AI Tools",
        Category.GUIDE: "AI Guide",
    }
    category_name = category_map.get(category, "AI News")

    # PaperModãƒ†ãƒ¼ãƒã¯cover.imageã‚’ä½¿ç”¨
    fm_lines = [
        "---",
        f'title: "{title.replace(chr(34), "")}"',
        f"date: {date_jst.isoformat()}",
        "cover:",
        f'  image: "{image_url}"',
        "  alt: \"AI generated thumbnail\"",
        "  relative: false",
        "categories:",
        f'  - "{category_name}"',
        "tags:",
        *[f'  - "{t}"' for t in tags],
        "---",
        "",
    ]

    content = "\n".join(fm_lines) + body.strip() + "\n"
    out_path.write_text(content, encoding="utf-8")


# ============================================================
# Twitter Posting Queue (ãƒ‡ãƒ—ãƒ­ã‚¤å¾ŒæŠ•ç¨¿ç”¨)
# ============================================================

@dataclass
class TwitterQueueItem:
    """XæŠ•ç¨¿ã‚­ãƒ¥ãƒ¼ã®ã‚¢ã‚¤ãƒ†ãƒ """
    article_id: str
    title: str
    url: str
    category: str  # "NEWS", "TOOL", "GUIDE"
    viral_tags: Optional[str]
    created_at: str


class TwitterPostingQueue:
    """XæŠ•ç¨¿ã‚­ãƒ¥ãƒ¼ã®ç®¡ç†ï¼ˆJSONæ°¸ç¶šåŒ–ï¼‰"""

    def __init__(self, path: Path) -> None:
        self.path = path
        self._queue: List[Dict] = []

    def load(self) -> List[Dict]:
        if not self.path.exists():
            self._queue = []
            return self._queue
        try:
            data = json.loads(self.path.read_text(encoding="utf-8"))
            self._queue = data if isinstance(data, list) else []
        except Exception:
            self._queue = []
        return self._queue

    def add(self, item: TwitterQueueItem) -> None:
        if not self._queue:
            self.load()
        self._queue.append({
            "article_id": item.article_id,
            "title": item.title,
            "url": item.url,
            "category": item.category,
            "viral_tags": item.viral_tags,
            "created_at": item.created_at,
            "posted": False,
        })
        self._save()

    def get_pending(self) -> List[Dict]:
        if not self._queue:
            self.load()
        return [item for item in self._queue if not item.get("posted", False)]

    def mark_posted(self, article_id: str) -> None:
        if not self._queue:
            self.load()
        for item in self._queue:
            if item["article_id"] == article_id:
                item["posted"] = True
        self._save()

    def get_by_id(self, article_id: str) -> Optional[Dict]:
        if not self._queue:
            self.load()
        for item in self._queue:
            if item["article_id"] == article_id:
                return item
        return None

    def _save(self) -> None:
        self.path.parent.mkdir(parents=True, exist_ok=True)
        self.path.write_text(
            json.dumps(self._queue, ensure_ascii=False, indent=2) + "\n",
            encoding="utf-8",
        )


def post_single_article_to_twitter(article_id: str) -> int:
    """
    æŒ‡å®šã—ãŸè¨˜äº‹IDã‚’Xã«æŠ•ç¨¿ã™ã‚‹ï¼ˆãƒ‡ãƒ—ãƒ­ã‚¤å®Œäº†å¾Œã«ä½¿ç”¨ï¼‰ã€‚
    
    Usage: python auto_generate.py --post-twitter 2026-01-14-abc12345
    """
    print("=" * 60)
    print("Negi AI Lab - Post to X (Twitter)")
    print("=" * 60)
    print(f"Article ID: {article_id}")
    print()

    # ã‚­ãƒ¥ãƒ¼ã‹ã‚‰è¨˜äº‹æƒ…å ±ã‚’å–å¾—
    repo_root = Path(__file__).resolve().parent
    queue = TwitterPostingQueue(repo_root / "twitter_queue.json")
    queue.load()

    item = queue.get_by_id(article_id)
    if not item:
        print(f"[ERROR] Article ID '{article_id}' not found in queue.")
        print("  Run article generation first, or check twitter_queue.json")
        return 1

    if item.get("posted", False):
        print(f"[WARN] Article already posted to X.")
        return 0

    # Twitterèªè¨¼ãƒã‚§ãƒƒã‚¯
    if not is_twitter_configured():
        print("[ERROR] Twitter API credentials not configured.")
        return 2

    if not TWEEPY_AVAILABLE:
        print("[ERROR] tweepy is not installed.")
        return 2

    try:
        poster = TwitterPoster()
        
        # ã‚«ãƒ†ã‚´ãƒªãƒ¼æ–‡å­—åˆ—ã‚’Enumã«å¤‰æ›
        category_map = {
            "NEWS": Category.NEWS,
            "TOOL": Category.TOOL,
            "GUIDE": Category.GUIDE,
        }
        category = category_map.get(item["category"], Category.NEWS)

        success = poster.post_article(
            title=item["title"],
            url=item["url"],
            category=category,
            viral_tags=item.get("viral_tags"),
        )

        if success:
            queue.mark_posted(article_id)
            print(f"âœ“ Successfully posted to X!")
            print(f"  Title: {item['title'][:50]}...")
            print(f"  URL: {item['url']}")
            return 0
        else:
            print("âœ— Failed to post to X.")
            return 1

    except Exception as e:
        print(f"[ERROR] {e}")
        return 1


def post_all_pending_to_twitter() -> int:
    """
    ã‚­ãƒ¥ãƒ¼å†…ã®æœªæŠ•ç¨¿è¨˜äº‹ã‚’ã™ã¹ã¦Xã«æŠ•ç¨¿ã™ã‚‹ã€‚
    
    Usage: python auto_generate.py --post-all-twitter
    """
    print("=" * 60)
    print("Negi AI Lab - Post All Pending to X")
    print("=" * 60)

    repo_root = Path(__file__).resolve().parent
    queue = TwitterPostingQueue(repo_root / "twitter_queue.json")
    pending = queue.get_pending()

    if not pending:
        print("No pending articles to post.")
        return 0

    print(f"Found {len(pending)} pending articles.")
    print()

    if not is_twitter_configured() or not TWEEPY_AVAILABLE:
        print("[ERROR] Twitter not configured or tweepy not installed.")
        return 2

    poster = TwitterPoster()
    success_count = 0

    for item in pending:
        print(f"Posting: {item['title'][:40]}...")
        
        category_map = {
            "NEWS": Category.NEWS,
            "TOOL": Category.TOOL,
            "GUIDE": Category.GUIDE,
        }
        category = category_map.get(item["category"], Category.NEWS)

        if poster.post_article(
            title=item["title"],
            url=item["url"],
            category=category,
            viral_tags=item.get("viral_tags"),
        ):
            queue.mark_posted(item["article_id"])
            print(f"  âœ“ Posted!")
            success_count += 1
        else:
            print(f"  âœ— Failed")
        
        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
        time.sleep(5)

    print()
    print(f"Posted {success_count}/{len(pending)} articles.")
    return 0 if success_count == len(pending) else 1


# ============================================================
# Fallback Logic Calculator
# ============================================================

def calculate_targets_with_fallback(
    total: int,
    available: Dict[Category, int],
) -> Dict[Category, int]:
    """
    ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ­ã‚¸ãƒƒã‚¯ä»˜ãã®ç›®æ¨™æ•°è¨ˆç®—ã€‚

    Args:
        total: åˆè¨ˆç›®æ¨™è¨˜äº‹æ•°
        available: å„ã‚«ãƒ†ã‚´ãƒªãƒ¼ã§åˆ©ç”¨å¯èƒ½ãªè¨˜äº‹æ•°

    Returns:
        å„ã‚«ãƒ†ã‚´ãƒªãƒ¼ã§å®Ÿéš›ã«ç”Ÿæˆã™ã‚‹è¨˜äº‹æ•°
    """
    # åˆæœŸç›®æ¨™
    targets = {
        Category.NEWS: int(total * CATEGORY_RATIOS["NEWS"]),
        Category.TOOL: int(total * CATEGORY_RATIOS["TOOL"]),
        Category.GUIDE: total,  # GUIDEã¯æ®‹ã‚Šå…¨éƒ¨ã‚’å—ã‘æŒã¤
    }

    # ç«¯æ•°èª¿æ•´ï¼ˆNEWSã¨TOOLã®åˆè¨ˆãŒtotalã‚’è¶…ãˆãªã„ã‚ˆã†ã«ï¼‰
    targets[Category.GUIDE] = total - targets[Category.NEWS] - targets[Category.TOOL]

    final = {}
    carryover = 0

    # å„ªå…ˆé †: NEWS â†’ TOOL â†’ GUIDE
    for cat in [Category.NEWS, Category.TOOL, Category.GUIDE]:
        target_with_carry = targets[cat] + carryover
        actual = min(target_with_carry, available.get(cat, 0))
        final[cat] = actual
        carryover = target_with_carry - actual

    return final


# ============================================================
# Main Entry Point
# ============================================================

def main() -> int:
    parser = argparse.ArgumentParser(
        description="Negi AI Lab - Auto Article Generator",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument(
        "--total", "-n",
        type=int,
        default=DEFAULT_TOTAL_ARTICLES,
        help=f"åˆè¨ˆç”Ÿæˆè¨˜äº‹æ•° (default: {DEFAULT_TOTAL_ARTICLES})",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="APIã‚’å©ã‹ãšã€åé›†ã¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨ˆç®—ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ã¿å®Ÿè¡Œ",
    )
    parser.add_argument(
        "--test-one",
        action="store_true",
        help="æœ€åˆã®1ä»¶ã ã‘å®Ÿéš›ã«ç”Ÿæˆã—ã¦å“è³ªç¢ºèª",
    )
    parser.add_argument(
        "--skip-twitter",
        action="store_true",
        help="X (Twitter) ã¸ã®æŠ•ç¨¿ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆGitHub Actionsç”¨ï¼‰",
    )
    parser.add_argument(
        "--post-twitter",
        type=str,
        metavar="ARTICLE_ID",
        help="æŒ‡å®šã—ãŸè¨˜äº‹IDã‚’Xã«æŠ•ç¨¿ï¼ˆãƒ‡ãƒ—ãƒ­ã‚¤å®Œäº†å¾Œã«ä½¿ç”¨ï¼‰",
    )
    parser.add_argument(
        "--post-all-twitter",
        action="store_true",
        help="ã‚­ãƒ¥ãƒ¼å†…ã®æœªæŠ•ç¨¿è¨˜äº‹ã‚’ã™ã¹ã¦Xã«æŠ•ç¨¿",
    )

    args = parser.parse_args()
    
    # --post-twitter ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯å°‚ç”¨å‡¦ç†
    if args.post_twitter:
        return post_single_article_to_twitter(args.post_twitter)
    
    # --post-all-twitter ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯å°‚ç”¨å‡¦ç†
    if args.post_all_twitter:
        return post_all_pending_to_twitter()
    
    total = args.total
    dry_run = args.dry_run
    test_one = args.test_one
    skip_twitter = args.skip_twitter

    print("=" * 60)
    print("Negi AI Lab - Auto Article Generator")
    print("=" * 60)
    print(f"Total target: {total} articles")
    print(f"Mode: {'DRY-RUN' if dry_run else ('TEST-ONE' if test_one else 'FULL')}")
    print()

    repo_root = Path(__file__).resolve().parent
    processed_store = ProcessedURLStore(repo_root / "processed_urls.json")
    processed_store.load()

    # API Key check (dry-runä»¥å¤–ã§å¿…é ˆ)
    api_key = os.environ.get("GEMINI_API_KEY", "")
    if not api_key and not dry_run:
        print("[ERROR] GEMINI_API_KEY is not set.")
        return 2

    # -------------------------
    # Step 1: Collect items
    # -------------------------
    print("[Step 1] Collecting news items...")
    collector = NewsCollector(processed_store)

    news_items = collector.collect_news(max_items=total)
    tool_items = collector.collect_tools(max_items=total)
    guide_items = collector.collect_guides(max_items=total)

    available = {
        Category.NEWS: len(news_items),
        Category.TOOL: len(tool_items),
        Category.GUIDE: len(guide_items),
    }

    print(f"  NEWS:  {available[Category.NEWS]} items found")
    print(f"  TOOL:  {available[Category.TOOL]} items found")
    print(f"  GUIDE: {available[Category.GUIDE]} items found")
    print()

    # -------------------------
    # Step 2: Calculate with fallback
    # -------------------------
    print("[Step 2] Calculating targets with fallback...")
    targets = calculate_targets_with_fallback(total, available)

    print(f"  NEWS:  {targets[Category.NEWS]} (target: {int(total * 0.4)})")
    print(f"  TOOL:  {targets[Category.TOOL]} (target: {int(total * 0.4)})")
    print(f"  GUIDE: {targets[Category.GUIDE]} (target: {total - int(total * 0.4) * 2})")
    print(f"  Total: {sum(targets.values())}")
    print()

    # -------------------------
    # Build final item list
    # -------------------------
    final_items: List[NewsItem] = []
    final_items.extend(news_items[:targets[Category.NEWS]])
    final_items.extend(tool_items[:targets[Category.TOOL]])
    final_items.extend(guide_items[:targets[Category.GUIDE]])

    # -------------------------
    # Dry-run: Show simulation and exit
    # -------------------------
    if dry_run:
        print("[DRY-RUN] Simulation Results:")
        print("-" * 60)
        for i, item in enumerate(final_items, 1):
            print(f"{i:2}. [{item.category.value:5}] {item.title[:50]}...")
            print(f"     Source: {item.source}")
            print(f"     URL: {item.url[:60]}...")
            print(f"     [DryRun] Would post to X: {item.title[:40]}...")
            print()
        print("-" * 60)
        print(f"Total: {len(final_items)} articles would be generated.")
        print("(No files written, no URLs logged, no X posts)")
        return 0

    # -------------------------
    # Step 3: Generate articles
    # -------------------------
    if not final_items:
        print("[!] No items to generate.")
        return 0

    generator = ArticleGenerator(api_key=api_key, model_name="gemini-3-flash-preview")
    image_handler = ImageHandler(api_key=api_key, model_name="gemini-3-flash-preview")
    out_dir = repo_root / "content" / "posts"
    static_dir = repo_root / "static"
    images_dir = static_dir / "images" / "posts"

    # Twitter Poster (optional)
    twitter_poster: Optional[TwitterPoster] = None
    twitter_enabled = is_twitter_configured() and TWEEPY_AVAILABLE
    if twitter_enabled:
        try:
            twitter_poster = TwitterPoster()
            print("[Twitter] API configured and ready.")
        except Exception as e:
            print(f"[Twitter] Disabled: {e}")
            twitter_poster = None
    else:
        if not TWEEPY_AVAILABLE:
            print("[Twitter] Disabled: tweepy not installed")
        else:
            print("[Twitter] Disabled: API credentials not configured")

    # test-one: 1ä»¶ã ã‘
    if test_one:
        final_items = final_items[:1]
        print("[TEST-ONE] Generating 1 article only...")
    else:
        print(f"[Step 3] Generating {len(final_items)} articles...")

    print()

    # ã‚µã‚¤ãƒˆã®ãƒ™ãƒ¼ã‚¹URL (ãƒ‡ãƒ—ãƒ­ã‚¤å¾Œã®URL)
    base_url = "https://ai.negi-lab.com"

    success_count = 0
    twitter_success = 0
    for idx, item in enumerate(final_items, 1):
        print(f"[{idx}/{len(final_items)}] Generating: {item.title[:40]}...")

        try:
            # Generate article (returns ArticleResult)
            result = generator.generate_article(item)

            # Prepare output
            now_jst = datetime.now(JST)
            date_midnight = now_jst.replace(hour=0, minute=0, second=0, microsecond=0)
            ymd = date_midnight.strftime("%Y-%m-%d")

            article_id = f"{ymd}-{uuid.uuid4().hex[:8]}"
            filename = f"{article_id}.md"
            out_path = out_dir / filename

            # Generate and save image locally
            print(f"  Generating image...")
            image_path = image_handler.generate_and_save_image(
                title=result.title,
                body=result.body,
                category=item.category,
                article_id=article_id,
                output_dir=images_dir,
            )

            # Determine tags for Hugo front matter
            tags = ["GenAI"]
            if item.category == Category.NEWS:
                tags.extend(["é€Ÿå ±", "AIãƒ‹ãƒ¥ãƒ¼ã‚¹"])
            elif item.category == Category.TOOL:
                tags.extend(["ãƒ„ãƒ¼ãƒ«", "ãƒ¬ãƒ“ãƒ¥ãƒ¼"])
            else:
                tags.extend(["ã‚¬ã‚¤ãƒ‰", "ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«"])

            # Write file
            write_hugo_markdown(
                out_path=out_path,
                title=result.title,
                date_jst=date_midnight,
                image_url=image_path,
                category=item.category,
                tags=tags,
                body=result.body,
            )

            # Mark as processed
            processed_store.add(item.url)
            processed_store.save()

            # ãƒ­ã‚°å‡ºåŠ›
            print(f"  âœ“ Saved: {filename}")
            if result.shopping_keyword:
                print(f"    Shopping keyword: {result.shopping_keyword}")
            if result.viral_tags:
                print(f"    Viral tags: {result.viral_tags}")
            success_count += 1

            # XæŠ•ç¨¿ç”¨ã®ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ ï¼ˆãƒ‡ãƒ—ãƒ­ã‚¤å¾ŒæŠ•ç¨¿ç”¨ï¼‰
            article_url = f"{base_url}/posts/{article_id}/"
            twitter_queue = TwitterPostingQueue(repo_root / "twitter_queue.json")
            twitter_queue.add(TwitterQueueItem(
                article_id=article_id,
                title=result.title,
                url=article_url,
                category=item.category.value,
                viral_tags=result.viral_tags,
                created_at=now_jst.isoformat(),
            ))
            print(f"    Queued for X posting: {article_id}")

            # TwitteræŠ•ç¨¿ï¼ˆ--skip-twitter ã§ãªã‘ã‚Œã°å³æ™‚æŠ•ç¨¿ï¼‰
            if twitter_poster and not skip_twitter:
                if twitter_poster.post_article(
                    title=result.title,
                    url=article_url,
                    category=item.category,
                    viral_tags=result.viral_tags,
                ):
                    twitter_queue.mark_posted(article_id)
                    print(f"  âœ“ Posted to X (Twitter Card)")
                    twitter_success += 1
                else:
                    print(f"  âœ— X post failed (queued for later)")
            elif skip_twitter:
                print(f"    [--skip-twitter] X posting skipped (use --post-twitter {article_id} later)")

            # Rate limit
            if idx < len(final_items):
                print(f"  (Sleeping {SLEEP_SECONDS_PER_ARTICLE}s for rate limit...)")
                time.sleep(SLEEP_SECONDS_PER_ARTICLE)

        except Exception as e:
            print(f"  âœ— Failed: {e}")
            continue

    print()
    print("=" * 60)
    print(f"Done. Generated {success_count}/{len(final_items)} articles.")
    if twitter_poster and not skip_twitter:
        print(f"      Posted to X: {twitter_success}/{success_count}")
    elif skip_twitter:
        print(f"      X posting skipped. Run with --post-all-twitter after deploy.")
    print("=" * 60)

    return 0 if success_count > 0 else 1


if __name__ == "__main__":
    raise SystemExit(main())
