#!/usr/bin/env python3
"""
Negi AI Lab - Auto Article Generator
=====================================
AIãƒˆãƒ¬ãƒ³ãƒ‰ç‰¹åŒ–å‹ã®å…¨è‡ªå‹•ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆè¨˜äº‹ç”Ÿæˆã‚¨ãƒ³ã‚¸ãƒ³

ã‚«ãƒ†ã‚´ãƒªãƒ¼æ§‹æˆæ¯”ç‡:
  - NEWS (é€Ÿå ±): 40%
  - TOOL (ãƒ„ãƒ¼ãƒ«æ¤œè¨¼): 40%
  - GUIDE (è§£èª¬/ã‚¬ã‚¤ãƒ‰): 20%

ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½ã«ã‚ˆã‚Šã€ãƒã‚¿ä¸è¶³æ™‚ã¯æ¬¡ã‚«ãƒ†ã‚´ãƒªãƒ¼ã§è£œå¡«ã—ã€
å¿…ãšæŒ‡å®šã•ã‚ŒãŸåˆè¨ˆè¨˜äº‹æ•°ã‚’ç¢ºä¿ã™ã‚‹ã€‚

Usage:
  python auto_generate.py --dry-run          # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ã¿
  python auto_generate.py --test-one         # 1ä»¶ã ã‘ç”Ÿæˆã—ã¦ãƒ†ã‚¹ãƒˆ
  python auto_generate.py --total 10         # 10è¨˜äº‹ç”Ÿæˆ
"""

from __future__ import annotations

import argparse
import io
import json
import os
import random
import re
import time
import uuid
from dataclasses import dataclass, field
from datetime import datetime, timedelta, timezone
from enum import Enum
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from urllib.parse import quote, urlencode

import feedparser
import requests
from bs4 import BeautifulSoup

import google.generativeai as genai

# TwitteræŠ•ç¨¿ç”¨ (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)
try:
    import tweepy
    TWEEPY_AVAILABLE = True
except ImportError:
    TWEEPY_AVAILABLE = False

# Optional: load .env if available
try:
    from dotenv import load_dotenv
    load_dotenv()
except Exception:
    pass


# ============================================================
# Constants & Configuration
# ============================================================

JST = timezone(timedelta(hours=9))
SLEEP_SECONDS_PER_ARTICLE = 30
DEFAULT_TOTAL_ARTICLES = 10

# ã‚«ãƒ†ã‚´ãƒªãƒ¼ç›®æ¨™æ¯”ç‡
CATEGORY_RATIOS = {
    "NEWS": 0.4,
    "TOOL": 0.4,
    "GUIDE": 0.2,
}

# User-Agent for requests
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"


class Category(Enum):
    NEWS = "NEWS"
    TOOL = "TOOL"
    GUIDE = "GUIDE"


@dataclass
class NewsItem:
    """åé›†ã—ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¢ã‚¤ãƒ†ãƒ """
    source: str
    title: str
    url: str
    category: Category
    published: str = ""
    summary: str = ""
    extra: Dict = field(default_factory=dict)


# ============================================================
# Processed URLs Store
# ============================================================

class ProcessedURLStore:
    """é‡è¤‡é˜²æ­¢ç”¨ã®URLæ°¸ç¶šåŒ–ã‚¹ãƒˆã‚¢"""

    def __init__(self, path: Path) -> None:
        self.path = path
        self._urls: List[str] = []
        self._dirty = False

    def load(self) -> List[str]:
        if not self.path.exists():
            self._urls = []
            return self._urls
        try:
            data = json.loads(self.path.read_text(encoding="utf-8"))
            self._urls = [str(x) for x in data] if isinstance(data, list) else []
        except Exception:
            self._urls = []
        return self._urls

    def contains(self, url: str) -> bool:
        if not self._urls:
            self.load()
        return url in self._urls

    def add(self, url: str) -> None:
        if not self._urls:
            self.load()
        if url not in self._urls:
            self._urls.append(url)
            self._dirty = True

    def save(self) -> None:
        if self._dirty:
            self.path.parent.mkdir(parents=True, exist_ok=True)
            self.path.write_text(
                json.dumps(self._urls, ensure_ascii=False, indent=2) + "\n",
                encoding="utf-8",
            )
            self._dirty = False


# ============================================================
# News Collector
# ============================================================

class NewsCollector:
    """å„ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""

    def __init__(self, processed_store: ProcessedURLStore) -> None:
        self.processed_store = processed_store
        self.session = requests.Session()
        self.session.headers.update({"User-Agent": USER_AGENT})

    def _is_fresh(self, url: str) -> bool:
        """URLãŒæœªå‡¦ç†ã‹ã©ã†ã‹"""
        return not self.processed_store.contains(url)

    def _normalize_text(self, text: str) -> str:
        return re.sub(r"\s+", " ", text).strip()

    # -------------------------
    # NEWS Sources
    # -------------------------

    def collect_news(self, max_items: int = 20) -> List[NewsItem]:
        """NEWS ã‚«ãƒ†ã‚´ãƒªãƒ¼: Google News RSS ã‹ã‚‰åé›†"""
        items: List[NewsItem] = []
        items.extend(self._collect_google_news(max_items))
        return items

    def _collect_google_news(self, max_items: int) -> List[NewsItem]:
        query = "Artificial Intelligence OR Gemini OR OpenAI OR Claude"
        rss_url = (
            "https://news.google.com/rss/search?"
            + urlencode({"q": query, "hl": "ja", "gl": "JP", "ceid": "JP:ja"})
        )

        try:
            resp = self.session.get(rss_url, timeout=20)
            resp.raise_for_status()
        except Exception as e:
            print(f"  [!] Google News RSS fetch failed: {e}")
            return []

        feed = feedparser.parse(resp.content)
        entries = getattr(feed, "entries", []) or []

        results: List[NewsItem] = []
        for entry in entries[:max_items * 2]:  # é‡è¤‡è€ƒæ…®ã—ã¦å¤šã‚ã«å–å¾—
            url = (getattr(entry, "link", "") or "").strip()
            title = (getattr(entry, "title", "") or "").strip()
            published = (getattr(entry, "published", "") or "").strip()
            summary = self._normalize_text(getattr(entry, "summary", "") or "")

            if not url or not title:
                continue
            if not self._is_fresh(url):
                continue

            results.append(NewsItem(
                source="Google News",
                title=title,
                url=url,
                category=Category.NEWS,
                published=published,
                summary=summary[:500],
            ))

            if len(results) >= max_items:
                break

        return results

    # -------------------------
    # TOOL Sources
    # -------------------------

    def collect_tools(self, max_items: int = 20) -> List[NewsItem]:
        """TOOL ã‚«ãƒ†ã‚´ãƒªãƒ¼: Product Hunt + GitHub Trending ã‹ã‚‰åé›†"""
        items: List[NewsItem] = []

        # Product Hunt (åŠåˆ†)
        ph_items = self._collect_product_hunt(max_items // 2 + 2)
        items.extend(ph_items)

        # GitHub Trending (æ®‹ã‚Š)
        remaining = max_items - len(items) + 5
        gh_items = self._collect_github_trending(remaining)
        items.extend(gh_items)

        return items[:max_items]

    def _collect_product_hunt(self, max_items: int) -> List[NewsItem]:
        """Product Hunt RSS ã‹ã‚‰ AIé–¢é€£ãƒ„ãƒ¼ãƒ«ã‚’åé›†"""
        rss_url = "https://www.producthunt.com/feed"

        try:
            resp = self.session.get(rss_url, timeout=20)
            resp.raise_for_status()
        except Exception as e:
            print(f"  [!] Product Hunt RSS fetch failed: {e}")
            return []

        feed = feedparser.parse(resp.content)
        entries = getattr(feed, "entries", []) or []

        # AIé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿
        ai_keywords = ["ai", "gpt", "llm", "machine learning", "ml", "neural",
                       "copilot", "chatbot", "automation", "generative"]

        results: List[NewsItem] = []
        for entry in entries:
            url = (getattr(entry, "link", "") or "").strip()
            title = (getattr(entry, "title", "") or "").strip()
            summary = self._normalize_text(getattr(entry, "summary", "") or "")

            if not url or not title:
                continue

            # AIé–¢é€£ãƒã‚§ãƒƒã‚¯
            text_lower = (title + " " + summary).lower()
            if not any(kw in text_lower for kw in ai_keywords):
                continue

            if not self._is_fresh(url):
                continue

            results.append(NewsItem(
                source="Product Hunt",
                title=title,
                url=url,
                category=Category.TOOL,
                summary=summary[:500],
            ))

            if len(results) >= max_items:
                break

        return results

    def _collect_github_trending(self, max_items: int) -> List[NewsItem]:
        """GitHub Trending (machine-learning) ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        url = "https://github.com/trending?since=daily&spoken_language_code=en"

        try:
            resp = self.session.get(url, timeout=20)
            resp.raise_for_status()
        except Exception as e:
            print(f"  [!] GitHub Trending fetch failed: {e}")
            return []

        soup = BeautifulSoup(resp.text, "html.parser")
        repo_list = soup.select("article.Box-row")

        results: List[NewsItem] = []
        for repo in repo_list[:max_items * 2]:
            try:
                h2 = repo.select_one("h2 a")
                if not h2:
                    continue

                repo_path = h2.get("href", "").strip()
                if not repo_path:
                    continue

                repo_url = f"https://github.com{repo_path}"
                repo_name = repo_path.strip("/")

                # Description
                desc_elem = repo.select_one("p")
                description = self._normalize_text(desc_elem.get_text()) if desc_elem else ""

                # Stars today
                stars_elem = repo.select_one("span.d-inline-block.float-sm-right")
                stars_today = self._normalize_text(stars_elem.get_text()) if stars_elem else ""

                if not self._is_fresh(repo_url):
                    continue

                results.append(NewsItem(
                    source="GitHub Trending",
                    title=repo_name,
                    url=repo_url,
                    category=Category.TOOL,
                    summary=description[:500],
                    extra={"stars_today": stars_today},
                ))

                if len(results) >= max_items:
                    break

            except Exception:
                continue

        return results

    # -------------------------
    # GUIDE Sources
    # -------------------------

    def collect_guides(self, max_items: int = 20) -> List[NewsItem]:
        """GUIDE ã‚«ãƒ†ã‚´ãƒªãƒ¼: Reddit + å®šç•ªãƒˆãƒ”ãƒƒã‚¯ã‹ã‚‰åé›†"""
        items: List[NewsItem] = []

        # Reddit
        reddit_items = self._collect_reddit(max_items)
        items.extend(reddit_items)

        # è¶³ã‚Šãªã„å ´åˆã¯å®šç•ªã‚¬ã‚¤ãƒ‰ãƒˆãƒ”ãƒƒã‚¯ã§è£œå¡«
        if len(items) < max_items:
            remaining = max_items - len(items)
            fallback_items = self._generate_guide_topics(remaining)
            items.extend(fallback_items)

        return items[:max_items]

    def _collect_reddit(self, max_items: int) -> List[NewsItem]:
        """Reddit (r/LocalLLaMA, r/OpenAI) ã® Top posts ã‚’åé›†"""
        subreddits = ["LocalLLaMA", "OpenAI", "MachineLearning", "artificial"]
        results: List[NewsItem] = []

        for subreddit in subreddits:
            if len(results) >= max_items:
                break

            # Reddit JSON API (èªè¨¼ä¸è¦)
            url = f"https://www.reddit.com/r/{subreddit}/top.json?t=week&limit=10"

            try:
                resp = self.session.get(url, timeout=20)
                resp.raise_for_status()
                data = resp.json()
            except Exception as e:
                print(f"  [!] Reddit r/{subreddit} fetch failed: {e}")
                continue

            posts = data.get("data", {}).get("children", [])
            for post in posts:
                post_data = post.get("data", {})
                title = post_data.get("title", "").strip()
                permalink = post_data.get("permalink", "")
                selftext = post_data.get("selftext", "")[:500]
                score = post_data.get("score", 0)

                if not title or not permalink:
                    continue

                post_url = f"https://www.reddit.com{permalink}"

                if not self._is_fresh(post_url):
                    continue

                # ã‚¹ã‚³ã‚¢ãŒä½ã™ãã‚‹ã‚‚ã®ã¯é™¤å¤–
                if score < 50:
                    continue

                results.append(NewsItem(
                    source=f"Reddit r/{subreddit}",
                    title=title,
                    url=post_url,
                    category=Category.GUIDE,
                    summary=selftext,
                    extra={"score": score},
                ))

                if len(results) >= max_items:
                    break

        return results

    def _generate_guide_topics(self, count: int) -> List[NewsItem]:
        """å®šç•ªã‚¬ã‚¤ãƒ‰ãƒˆãƒ”ãƒƒã‚¯ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰"""
        topics = [
            ("ãƒ­ãƒ¼ã‚«ãƒ«LLMã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¬ã‚¤ãƒ‰ï¼ˆ2024å¹´ç‰ˆï¼‰", "local-llm-setup"),
            ("Ollama + Open WebUIã§è‡ªå®…AIãƒãƒ£ãƒƒãƒˆã‚’æ§‹ç¯‰ã™ã‚‹æ–¹æ³•", "ollama-webui-setup"),
            ("RAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®åŸºç¤ã¨å®Ÿè£…å…¥é–€", "rag-pipeline-intro"),
            ("LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Œå…¨ã‚¬ã‚¤ãƒ‰", "lora-finetuning-guide"),
            ("LangChainã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä½œã‚‹æ–¹æ³•", "langchain-agent-tutorial"),
            ("Hugging Face Transformersã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ", "hf-transformers-quickstart"),
            ("GPT-4 APIã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹", "gpt4-api-best-practices"),
            ("Stable Diffusion XLãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°", "sdxl-prompt-engineering"),
            ("LLMã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’æœ€å¤§é™æ´»ç”¨ã™ã‚‹ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯", "llm-context-length-tips"),
            ("AIã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆå¾¹åº•æ¯”è¼ƒ", "ai-coding-assistant-comparison"),
            ("Gemini APIå…¥é–€ï¼šæœ€åˆã®ã‚¢ãƒ—ãƒªã‚’ä½œã‚‹", "gemini-api-getting-started"),
            ("Claude APIã®ä½¿ã„æ–¹ã¨Tips", "claude-api-tips"),
            ("ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹é¸å®šã‚¬ã‚¤ãƒ‰", "vector-db-comparison"),
            ("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³å¯¾ç­–å…¥é–€", "prompt-injection-defense"),
            ("ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«AIã®æ´»ç”¨äº‹ä¾‹é›†", "multimodal-ai-use-cases"),
        ]

        random.shuffle(topics)
        results: List[NewsItem] = []

        for title, slug in topics[:count]:
            # ä»®ã®URLã‚’ç”Ÿæˆï¼ˆå®Ÿéš›ã«ã¯URLãªã—ã§è¨˜äº‹ç”Ÿæˆï¼‰
            fake_url = f"guide://{slug}-{uuid.uuid4().hex[:6]}"

            if not self._is_fresh(fake_url):
                continue

            results.append(NewsItem(
                source="Negi Lab Guide",
                title=title,
                url=fake_url,
                category=Category.GUIDE,
                summary="",
                extra={"is_generated_topic": True},
            ))

        return results


# ============================================================
# Article Generator
# ============================================================

class ArticleGenerator:
    """Gemini APIã‚’ä½¿ç”¨ã—ã¦ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¥ã®è¨˜äº‹ã‚’ç”Ÿæˆ"""

    def __init__(self, api_key: str, model_name: str = "gemini-3-flash-preview") -> None:
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(model_name)

    def generate_article(self, item: NewsItem) -> Tuple[str, str]:
        """
        è¨˜äº‹ã‚’ç”Ÿæˆã—ã€(ã‚¿ã‚¤ãƒˆãƒ«, æœ¬æ–‡) ã‚’è¿”ã™ã€‚
        """
        prompt = self._build_prompt(item)

        try:
            response = self.model.generate_content(prompt)
        except Exception as e:
            raise RuntimeError(f"Gemini API error: {e}")

        text = getattr(response, "text", None)
        if not text:
            raise RuntimeError("Gemini returned empty response")

        # å¤–å´ã®```markdownãƒ•ã‚§ãƒ³ã‚¹ã®ã¿é™¤å»ï¼ˆå†…éƒ¨ã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã¯ä¿æŒï¼‰
        cleaned = text.strip()
        
        # å…ˆé ­ã®ãƒ•ã‚§ãƒ³ã‚¹é™¤å»
        if cleaned.startswith("```markdown"):
            cleaned = cleaned[len("```markdown"):].strip()
        elif cleaned.startswith("```md"):
            cleaned = cleaned[len("```md"):].strip()
        elif cleaned.startswith("```"):
            cleaned = cleaned[3:].strip()
        
        # æœ«å°¾ã®ãƒ•ã‚§ãƒ³ã‚¹é™¤å»ï¼ˆæœ¬æ–‡æœ«ã®å˜ç‹¬```ã®ã¿å¯¾è±¡ï¼‰
        lines = cleaned.split("\n")
        while lines and lines[-1].strip() == "```":
            lines.pop()
        cleaned = "\n".join(lines).strip()

        title, body = self._extract_title_and_body(cleaned)
        return title, body

    def _build_prompt(self, item: NewsItem) -> str:
        """ã‚«ãƒ†ã‚´ãƒªãƒ¼ã«å¿œã˜ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰"""

        if item.category == Category.NEWS:
            return self._build_news_prompt(item)
        elif item.category == Category.TOOL:
            return self._build_tool_prompt(item)
        else:  # GUIDE
            return self._build_guide_prompt(item)

    def _build_news_prompt(self, item: NewsItem) -> str:
        return f'''ã‚ãªãŸã¯ã€ŒNegi Labã€æ‰€å±ã®è¾›å£ã ãŒæŠ€è¡“ã«è©³ã—ã„AIç ”ç©¶å“¡ã§ã™ã€‚
èª­è€…ã¯ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã‚„ãƒ“ã‚¸ãƒã‚¹ãƒãƒ³ã€‚å®Ÿç”¨æ€§ã¨æŠ€è¡“çš„è¦–ç‚¹ã‚’é‡è¦–ã—ã¦åŸ·ç­†ã—ã¦ãã ã•ã„ã€‚

ã€å…ƒãƒ‹ãƒ¥ãƒ¼ã‚¹æƒ…å ±ã€‘
- ã‚¿ã‚¤ãƒˆãƒ«: {item.title}
- URL: {item.url}
- å‡ºå…¸: {item.source}
{f"- è¦ç´„: {item.summary}" if item.summary else ""}

ã€æŒ‡ç¤ºã€‘
é€Ÿå ±ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚
- ä½•ãŒç™ºè¡¨ã•ã‚ŒãŸã‹
- ç«¶åˆï¼ˆChatGPT, Claudeç­‰ï¼‰ã¨ã®é•ã„ã¯ä½•ã‹
- æ¥­ç•Œã¸ã®å½±éŸ¿ã‚’è«–ç†çš„ã«è§£èª¬

ã€å‡ºåŠ›å½¢å¼ã€‘å¿…ãšä»¥ä¸‹ã®Markdownæ§‹é€ ã§å‡ºåŠ›ã™ã‚‹ã“ã¨ã€‚

1è¡Œç›®: ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆè£…é£¾ãªã—ã€## ãªã©ã¯ä»˜ã‘ãªã„ï¼‰

## 3è¡Œè¦ç´„

- è¦ç´„1
- è¦ç´„2
- è¦ç´„3

## ä½•ãŒç™ºè¡¨ã•ã‚ŒãŸã®ã‹

ï¼ˆè©³ç´°ãªè§£èª¬ï¼‰

## ç«¶åˆã¨ã®æ¯”è¼ƒ

| é …ç›® | ä»Šå›ã®ç™ºè¡¨ | ChatGPT | Claude |
|------|-----------|---------|--------|
| ... | ... | ... | ... |

## æ¥­ç•Œã¸ã®å½±éŸ¿

ï¼ˆè«–ç†çš„ãªåˆ†æï¼‰

## Negi Labã®è¦‹è§£

ï¼ˆè¾›å£ã ãŒå»ºè¨­çš„ãªã‚³ãƒ¡ãƒ³ãƒˆï¼‰

ã€ç¦æ­¢äº‹é …ã€‘
- YAML/TOML Front Matterã¯å‡ºåŠ›ã—ãªã„
- HTMLã‚¿ã‚°ã¯ä½¿ã‚ãªã„
'''

    def _build_tool_prompt(self, item: NewsItem) -> str:
        stars_info = f"- ä»Šæ—¥ã®ã‚¹ã‚¿ãƒ¼æ•°: {item.extra['stars_today']}" if item.extra.get("stars_today") else ""

        return f'''ã‚ãªãŸã¯ã€ŒNegi Labã€æ‰€å±ã®è¾›å£ã ãŒæŠ€è¡“ã«è©³ã—ã„AIç ”ç©¶å“¡ã§ã™ã€‚
èª­è€…ã¯ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã‚„ãƒ“ã‚¸ãƒã‚¹ãƒãƒ³ã€‚å®Ÿç”¨æ€§ã¨æŠ€è¡“çš„è¦–ç‚¹ã‚’é‡è¦–ã—ã¦åŸ·ç­†ã—ã¦ãã ã•ã„ã€‚

ã€å¯¾è±¡ãƒ„ãƒ¼ãƒ«æƒ…å ±ã€‘
- åå‰: {item.title}
- URL: {item.url}
- å‡ºå…¸: {item.source}
{f"- èª¬æ˜: {item.summary}" if item.summary else ""}
{stars_info}

ã€æŒ‡ç¤ºã€‘
ã€Œ**æ¤œè¨¼ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³**ã€ã‚’å«ã‚€ãƒ„ãƒ¼ãƒ«ãƒ¬ãƒ“ãƒ¥ãƒ¼è¨˜äº‹ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚
å®Ÿéš›ã«ãã®ãƒ„ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦å‹•ã‹ã—ãŸã¨ä»®å®šã—ã€ä»¥ä¸‹ã‚’å«ã‚ã‚‹ã“ã¨ï¼š
- å…·ä½“çš„ãªPythonã‚³ãƒ¼ãƒ‰ä¾‹ï¼ˆå‹•ä½œã™ã‚‹é¢¨ã®ã‚³ãƒ¼ãƒ‰ï¼‰
- å®Ÿè¡Œçµæœã®ä¾‹ï¼ˆæ¶ç©ºã§OKï¼‰
- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¾‹ï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰

â€»è¨˜äº‹å†’é ­ã«ã€Œã“ã‚Œã¯ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã‚ã‚Šå®Ÿæ¸¬ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€ã¨æ˜è¨˜ã™ã‚‹ã“ã¨ã€‚

ã€å‡ºåŠ›å½¢å¼ã€‘å¿…ãšä»¥ä¸‹ã®Markdownæ§‹é€ ã§å‡ºåŠ›ã™ã‚‹ã“ã¨ã€‚

1è¡Œç›®: ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆè£…é£¾ãªã—ã€ã‚­ãƒ£ãƒƒãƒãƒ¼ã«ï¼‰

> âš ï¸ æœ¬è¨˜äº‹ã®æ¤œè¨¼ãƒ‘ãƒ¼ãƒˆã¯ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã‚ã‚Šã€å®Ÿéš›ã®æ¸¬å®šçµæœã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

## 3è¡Œè¦ç´„

- è¦ç´„1
- è¦ç´„2
- è¦ç´„3

## ã“ã®ãƒ„ãƒ¼ãƒ«ã¯ä½•ã‹

ï¼ˆæ¦‚è¦èª¬æ˜ï¼‰

## æ¤œè¨¼ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼šå®Ÿéš›ã«ä½¿ã£ã¦ã¿ãŸ

### ç’°å¢ƒæ§‹ç¯‰

```bash
pip install xxx
```

### åŸºæœ¬çš„ãªä½¿ã„æ–¹

```python
# ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ï¼ˆæ¶ç©ºã§OKï¼‰
from xxx import YYY

model = YYY()
result = model.run("ãƒ†ã‚¹ãƒˆå…¥åŠ›")
print(result)
```

### å®Ÿè¡Œçµæœ

```
ï¼ˆæ¶ç©ºã®å‡ºåŠ›ä¾‹ï¼‰
```

## ãƒ¡ãƒªãƒƒãƒˆãƒ»ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ

### ãƒ¡ãƒªãƒƒãƒˆ
- ...

### ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ
- ...

## çµè«–ï¼šNegi Labã®è©•ä¾¡

ï¼ˆæ˜Ÿè©•ä¾¡: â˜…â˜…â˜…â˜†â˜† ã®ã‚ˆã†ãªå½¢å¼ã‚‚å¯ï¼‰

ã€ç¦æ­¢äº‹é …ã€‘
- YAML/TOML Front Matterã¯å‡ºåŠ›ã—ãªã„
- HTMLã‚¿ã‚°ã¯ä½¿ã‚ãªã„
'''

    def _build_guide_prompt(self, item: NewsItem) -> str:
        is_generated = item.extra.get("is_generated_topic", False)
        topic_info = f"- ãƒˆãƒ”ãƒƒã‚¯: {item.title}" if is_generated else f'''- ã‚¿ã‚¤ãƒˆãƒ«: {item.title}
- URL: {item.url}
- å‡ºå…¸: {item.source}
{f"- å†…å®¹: {item.summary}" if item.summary else ""}'''

        return f'''ã‚ãªãŸã¯ã€ŒNegi Labã€æ‰€å±ã®è¾›å£ã ãŒæŠ€è¡“ã«è©³ã—ã„AIç ”ç©¶å“¡ã§ã™ã€‚
èª­è€…ã¯ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã‚„ãƒ“ã‚¸ãƒã‚¹ãƒãƒ³ã€‚å®Ÿç”¨æ€§ã¨æŠ€è¡“çš„è¦–ç‚¹ã‚’é‡è¦–ã—ã¦åŸ·ç­†ã—ã¦ãã ã•ã„ã€‚

ã€ãƒˆãƒ”ãƒƒã‚¯æƒ…å ±ã€‘
{topic_info}

ã€æŒ‡ç¤ºã€‘
ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«å½¢å¼ã®ã‚¬ã‚¤ãƒ‰è¨˜äº‹ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚
ã€Œã€œã™ã‚‹æ–¹æ³•ã€ã€Œã‚¨ãƒ©ãƒ¼å›é¿æ‰‹é †ã€ãªã©ã€èª­è€…ãŒæ‰‹å…ƒã§è©¦ã›ã‚‹ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ã®æ‰‹é †æ›¸ã¨ã—ã¦æ›¸ãã“ã¨ã€‚
- å…·ä½“çš„ãªã‚³ãƒãƒ³ãƒ‰ä¾‹
- ã‚³ãƒ¼ãƒ‰ä¾‹
- è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ä¾‹
ã‚’å¿…ãšå«ã‚ã‚‹ã“ã¨ã€‚

ã€å‡ºåŠ›å½¢å¼ã€‘å¿…ãšä»¥ä¸‹ã®Markdownæ§‹é€ ã§å‡ºåŠ›ã™ã‚‹ã“ã¨ã€‚

1è¡Œç›®: ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆè£…é£¾ãªã—ã€ã€Œã€œã™ã‚‹æ–¹æ³•ã€ã€Œã€œå…¥é–€ã€å½¢å¼æ¨å¥¨ï¼‰

## ã“ã®è¨˜äº‹ã§å­¦ã¹ã‚‹ã“ã¨

- ãƒã‚¤ãƒ³ãƒˆ1
- ãƒã‚¤ãƒ³ãƒˆ2
- ãƒã‚¤ãƒ³ãƒˆ3

## å‰ææ¡ä»¶

- å¿…è¦ãªã‚‚ã®1
- å¿…è¦ãªã‚‚ã®2

## Step 1: ç’°å¢ƒæº–å‚™

```bash
# ã‚³ãƒãƒ³ãƒ‰ä¾‹
```

## Step 2: åŸºæœ¬è¨­å®š

```python
# è¨­å®šã‚³ãƒ¼ãƒ‰ä¾‹
```

## Step 3: å®Ÿè¡Œã¨ç¢ºèª

ï¼ˆæ‰‹é †ã®èª¬æ˜ï¼‰

## ã‚ˆãã‚ã‚‹ã‚¨ãƒ©ãƒ¼ã¨å¯¾å‡¦æ³•

### ã‚¨ãƒ©ãƒ¼1: xxx

```
ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä¾‹
```

**è§£æ±ºç­–:** ...

## ã¾ã¨ã‚ã¨æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

ï¼ˆç· ã‚ã®è¨€è‘‰ã¨ã€æ¬¡ã«å­¦ã¶ã¹ãã“ã¨ã¸ã®èª˜å°ï¼‰

ã€ç¦æ­¢äº‹é …ã€‘
- YAML/TOML Front Matterã¯å‡ºåŠ›ã—ãªã„
- HTMLã‚¿ã‚°ã¯ä½¿ã‚ãªã„
'''

    def _extract_title_and_body(self, text: str) -> Tuple[str, str]:
        lines = [ln.rstrip() for ln in text.splitlines()]
        title = ""
        body_start = 0

        for i, ln in enumerate(lines):
            stripped = ln.strip()
            if stripped:
                # ã‚¿ã‚¤ãƒˆãƒ«è¡Œã‚’æŠ½å‡º
                title = stripped.strip('"').strip("'")
                title = re.sub(r"^#{1,6}\s+", "", title).strip()
                body_start = i + 1
                break

        if not title:
            title = "AIãƒˆãƒ¬ãƒ³ãƒ‰è¨˜äº‹ï¼ˆè‡ªå‹•ç”Ÿæˆï¼‰"

        body = "\n".join(lines[body_start:]).lstrip("\n")
        if not body:
            body = "(æœ¬æ–‡ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ)"

        return title, body


# ============================================================
# Image Handler
# ============================================================

class ImageHandler:
    """Pollinations.ai ã‚’ä½¿ç”¨ã—ãŸç”»åƒç”Ÿæˆãƒ»ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜"""

    def __init__(self, api_key: str, model_name: str = "gemini-3-flash-preview") -> None:
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(model_name)

    def generate_and_save_image(
        self,
        title: str,
        body: str,
        category: Category,
        article_id: str,
        output_dir: Path,
    ) -> str:
        """
        è¨˜äº‹å†…å®¹ã«åŸºã¥ã„ãŸç”»åƒã‚’ç”Ÿæˆã—ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜ã€‚
        
        Args:
            title: è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«
            body: è¨˜äº‹æœ¬æ–‡
            category: ã‚«ãƒ†ã‚´ãƒªãƒ¼
            article_id: è¨˜äº‹IDï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åç”¨ï¼‰
            output_dir: ç”»åƒä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (static/images/posts/)
        
        Returns:
            Hugoç”¨ã®ç›¸å¯¾ãƒ‘ã‚¹ (ä¾‹: /images/posts/2026-01-13-abc123.png)
        """
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
        prompt_en = self._generate_image_prompt(title, body, category)
        
        # Pollinations.aiã‹ã‚‰ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        encoded = quote(prompt_en, safe="")
        image_url = f"https://image.pollinations.ai/prompt/{encoded}?width=1200&height=630&nologo=true"
        
        try:
            response = requests.get(image_url, timeout=60)
            response.raise_for_status()
            
            # ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            filename = f"{article_id}.png"
            file_path = output_dir / filename
            file_path.write_bytes(response.content)
            
            # Hugoç”¨ç›¸å¯¾ãƒ‘ã‚¹
            return f"/images/posts/{filename}"
            
        except Exception as e:
            print(f"  [Image] Download failed: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: URLã‚’è¿”ã™ï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™æ™‚ç”¨ï¼‰
            return image_url

    def download_image_to_bytes(self, image_path_or_url: str, static_dir: Path) -> Optional[bytes]:
        """
        ç”»åƒã‚’ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦å–å¾—ï¼ˆTwitteræŠ•ç¨¿ç”¨ï¼‰ã€‚
        
        Args:
            image_path_or_url: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ï¼ˆ/images/posts/xxx.pngï¼‰ã¾ãŸã¯URL
            static_dir: staticãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
        
        Returns:
            ç”»åƒã®ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿
        """
        try:
            if image_path_or_url.startswith("http"):
                # URLã®å ´åˆã¯ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                response = requests.get(image_path_or_url, timeout=30)
                response.raise_for_status()
                return response.content
            else:
                # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã®å ´åˆã¯ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
                # /images/posts/xxx.png -> static/images/posts/xxx.png
                local_path = static_dir / image_path_or_url.lstrip("/")
                if local_path.exists():
                    return local_path.read_bytes()
                return None
        except Exception as e:
            print(f"  [Image] Failed to load: {e}")
            return None

    def _generate_image_prompt(self, title: str, body: str, category: Category) -> str:
        style_hint = {
            Category.NEWS: "news broadcast, breaking news style, professional, no text",
            Category.TOOL: "software interface, tech product, modern UI, no text",
            Category.GUIDE: "tutorial, educational diagram, clean design, no text",
        }.get(category, "technology")

        prompt = (
            "Create ONE English image generation prompt for a blog thumbnail.\n"
            "Rules:\n"
            "- Output ONE line only, no quotes\n"
            "- English only\n"
            "- MUST NOT include any text, logos, watermarks, QR codes, or UI elements\n"
            "- Focus on abstract visuals, technology imagery, or symbolic representations\n"
            f"- Style hint: {style_hint}\n"
            "- Make it modern, clean, professional\n\n"
            f"Blog title: {title}\n"
            f"Content preview: {body[:500]}\n"
        )

        try:
            response = self.model.generate_content(prompt)
            text = getattr(response, "text", None)
            if text:
                one_line = text.strip().splitlines()[0].strip().strip('"').strip("'")
                if one_line:
                    return one_line
        except Exception:
            pass

        # Fallback
        return f"Abstract futuristic technology visualization, {style_hint}, minimalist, high quality, 4k, no text"


# ============================================================
# Twitter (X) Poster
# ============================================================

class TwitterPoster:
    """X (Twitter) ã¸ã®è‡ªå‹•æŠ•ç¨¿ã‚’è¡Œã†ã‚¯ãƒ©ã‚¹"""

    def __init__(self) -> None:
        """
        ç’°å¢ƒå¤‰æ•°ã‹ã‚‰èªè¨¼æƒ…å ±ã‚’èª­ã¿è¾¼ã¿ã€Tweepy Clientã‚’åˆæœŸåŒ–ã€‚
        å¿…è¦ãªç’°å¢ƒå¤‰æ•°:
          - TWITTER_API_KEY
          - TWITTER_API_SECRET
          - TWITTER_ACCESS_TOKEN
          - TWITTER_ACCESS_TOKEN_SECRET
          - TWITTER_BEARER_TOKEN (v2 APIç”¨)
        """
        if not TWEEPY_AVAILABLE:
            raise RuntimeError("tweepy is not installed. Run: pip install tweepy")

        self.api_key = os.getenv("TWITTER_API_KEY", "")
        self.api_secret = os.getenv("TWITTER_API_SECRET", "")
        self.access_token = os.getenv("TWITTER_ACCESS_TOKEN", "")
        self.access_token_secret = os.getenv("TWITTER_ACCESS_TOKEN_SECRET", "")
        self.bearer_token = os.getenv("TWITTER_BEARER_TOKEN", "")

        if not all([self.api_key, self.api_secret, self.access_token, self.access_token_secret]):
            raise RuntimeError("Twitter API credentials not configured in environment variables")

        # v1.1 API (for media upload)
        auth = tweepy.OAuth1UserHandler(
            self.api_key,
            self.api_secret,
            self.access_token,
            self.access_token_secret,
        )
        self.api_v1 = tweepy.API(auth)

        # v2 API (for tweeting)
        self.client = tweepy.Client(
            bearer_token=self.bearer_token,
            consumer_key=self.api_key,
            consumer_secret=self.api_secret,
            access_token=self.access_token,
            access_token_secret=self.access_token_secret,
        )

    def post_article(
        self,
        title: str,
        url: str,
        image_data: Optional[bytes],
        category: Category,
        hashtags: Optional[List[str]] = None,
    ) -> bool:
        """
        è¨˜äº‹ã‚’Twitterã«æŠ•ç¨¿ã™ã‚‹ã€‚

        Args:
            title: è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«
            url: è¨˜äº‹ã®URL (ãƒ‡ãƒ—ãƒ­ã‚¤å¾Œã®URL)
            image_data: ç”»åƒã®ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿æ¸ˆã¿ï¼‰
            category: è¨˜äº‹ã‚«ãƒ†ã‚´ãƒªãƒ¼
            hashtags: è¿½åŠ ã™ã‚‹ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ãƒªã‚¹ãƒˆ

        Returns:
            æŠ•ç¨¿æˆåŠŸæ™‚True
        """
        try:
            # ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¥ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°
            default_tags = {
                Category.NEWS: ["AIé€Ÿå ±", "AIãƒ‹ãƒ¥ãƒ¼ã‚¹"],
                Category.TOOL: ["AIãƒ„ãƒ¼ãƒ«", "é–‹ç™ºè€…å‘ã‘"],
                Category.GUIDE: ["AIå…¥é–€", "ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«"],
            }

            tags = hashtags or default_tags.get(category, ["AI"])
            tag_str = " ".join([f"#{t}" for t in tags[:3]])  # æœ€å¤§3ã¤

            # ãƒ„ã‚¤ãƒ¼ãƒˆæœ¬æ–‡ã‚’ä½œæˆ (280æ–‡å­—åˆ¶é™ã‚’è€ƒæ…®)
            # ã‚¿ã‚¤ãƒˆãƒ«ã‚’çŸ­ç¸®
            max_title_len = 100
            short_title = title[:max_title_len] + "..." if len(title) > max_title_len else title

            tweet_text = f"ğŸ“¢ {short_title}\n\n{url}\n\n{tag_str} #NegiAILab"

            # ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            media_id = self._upload_image_from_bytes(image_data) if image_data else None

            # ãƒ„ã‚¤ãƒ¼ãƒˆæŠ•ç¨¿
            if media_id:
                self.client.create_tweet(text=tweet_text, media_ids=[media_id])
            else:
                self.client.create_tweet(text=tweet_text)

            return True

        except Exception as e:
            print(f"  [Twitter] Error: {e}")
            return False

    def _upload_image_from_bytes(self, image_data: bytes) -> Optional[str]:
        """
        ç”»åƒãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’Twitterã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã€‚

        Args:
            image_data: ç”»åƒã®ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿

        Returns:
            media_id (æˆåŠŸæ™‚) ã¾ãŸã¯ None
        """
        try:
            # BytesIOã§ãƒ•ã‚¡ã‚¤ãƒ«ãƒ©ã‚¤ã‚¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨ã—ã¦æ‰±ã†
            image_file = io.BytesIO(image_data)

            # Twitterã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (v1.1 API)
            media = self.api_v1.media_upload(filename="thumbnail.png", file=image_file)
            return str(media.media_id)

        except Exception as e:
            print(f"  [Twitter] Image upload failed: {e}")
            return None


def is_twitter_configured() -> bool:
    """Twitter APIèªè¨¼æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª"""
    required = [
        "TWITTER_API_KEY",
        "TWITTER_API_SECRET",
        "TWITTER_ACCESS_TOKEN",
        "TWITTER_ACCESS_TOKEN_SECRET",
    ]
    return all(os.getenv(key) for key in required)


# ============================================================
# Hugo Markdown Writer
# ============================================================

def write_hugo_markdown(
    out_path: Path,
    title: str,
    date_jst: datetime,
    image_url: str,
    category: Category,
    tags: List[str],
    body: str,
) -> None:
    """Hugoå½¢å¼ã®Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡ºåŠ›ï¼ˆPaperModãƒ†ãƒ¼ãƒå¯¾å¿œï¼‰"""
    out_path.parent.mkdir(parents=True, exist_ok=True)

    # ã‚«ãƒ†ã‚´ãƒªãƒ¼åã‚’Hugoç”¨ã«å¤‰æ›
    category_map = {
        Category.NEWS: "AI News",
        Category.TOOL: "AI Tools",
        Category.GUIDE: "AI Guide",
    }
    category_name = category_map.get(category, "AI News")

    # PaperModãƒ†ãƒ¼ãƒã¯cover.imageã‚’ä½¿ç”¨
    fm_lines = [
        "---",
        f'title: "{title.replace(chr(34), "")}"',
        f"date: {date_jst.isoformat()}",
        "cover:",
        f'  image: "{image_url}"',
        "  alt: \"AI generated thumbnail\"",
        "  relative: false",
        "categories:",
        f'  - "{category_name}"',
        "tags:",
        *[f'  - "{t}"' for t in tags],
        "---",
        "",
    ]

    content = "\n".join(fm_lines) + body.strip() + "\n"
    out_path.write_text(content, encoding="utf-8")


# ============================================================
# Fallback Logic Calculator
# ============================================================

def calculate_targets_with_fallback(
    total: int,
    available: Dict[Category, int],
) -> Dict[Category, int]:
    """
    ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ­ã‚¸ãƒƒã‚¯ä»˜ãã®ç›®æ¨™æ•°è¨ˆç®—ã€‚

    Args:
        total: åˆè¨ˆç›®æ¨™è¨˜äº‹æ•°
        available: å„ã‚«ãƒ†ã‚´ãƒªãƒ¼ã§åˆ©ç”¨å¯èƒ½ãªè¨˜äº‹æ•°

    Returns:
        å„ã‚«ãƒ†ã‚´ãƒªãƒ¼ã§å®Ÿéš›ã«ç”Ÿæˆã™ã‚‹è¨˜äº‹æ•°
    """
    # åˆæœŸç›®æ¨™
    targets = {
        Category.NEWS: int(total * CATEGORY_RATIOS["NEWS"]),
        Category.TOOL: int(total * CATEGORY_RATIOS["TOOL"]),
        Category.GUIDE: total,  # GUIDEã¯æ®‹ã‚Šå…¨éƒ¨ã‚’å—ã‘æŒã¤
    }

    # ç«¯æ•°èª¿æ•´ï¼ˆNEWSã¨TOOLã®åˆè¨ˆãŒtotalã‚’è¶…ãˆãªã„ã‚ˆã†ã«ï¼‰
    targets[Category.GUIDE] = total - targets[Category.NEWS] - targets[Category.TOOL]

    final = {}
    carryover = 0

    # å„ªå…ˆé †: NEWS â†’ TOOL â†’ GUIDE
    for cat in [Category.NEWS, Category.TOOL, Category.GUIDE]:
        target_with_carry = targets[cat] + carryover
        actual = min(target_with_carry, available.get(cat, 0))
        final[cat] = actual
        carryover = target_with_carry - actual

    return final


# ============================================================
# Main Entry Point
# ============================================================

def main() -> int:
    parser = argparse.ArgumentParser(
        description="Negi AI Lab - Auto Article Generator",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument(
        "--total", "-n",
        type=int,
        default=DEFAULT_TOTAL_ARTICLES,
        help=f"åˆè¨ˆç”Ÿæˆè¨˜äº‹æ•° (default: {DEFAULT_TOTAL_ARTICLES})",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="APIã‚’å©ã‹ãšã€åé›†ã¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨ˆç®—ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ã¿å®Ÿè¡Œ",
    )
    parser.add_argument(
        "--test-one",
        action="store_true",
        help="æœ€åˆã®1ä»¶ã ã‘å®Ÿéš›ã«ç”Ÿæˆã—ã¦å“è³ªç¢ºèª",
    )

    args = parser.parse_args()
    total = args.total
    dry_run = args.dry_run
    test_one = args.test_one

    print("=" * 60)
    print("Negi AI Lab - Auto Article Generator")
    print("=" * 60)
    print(f"Total target: {total} articles")
    print(f"Mode: {'DRY-RUN' if dry_run else ('TEST-ONE' if test_one else 'FULL')}")
    print()

    repo_root = Path(__file__).resolve().parent
    processed_store = ProcessedURLStore(repo_root / "processed_urls.json")
    processed_store.load()

    # API Key check (dry-runä»¥å¤–ã§å¿…é ˆ)
    api_key = os.environ.get("GEMINI_API_KEY", "")
    if not api_key and not dry_run:
        print("[ERROR] GEMINI_API_KEY is not set.")
        return 2

    # -------------------------
    # Step 1: Collect items
    # -------------------------
    print("[Step 1] Collecting news items...")
    collector = NewsCollector(processed_store)

    news_items = collector.collect_news(max_items=total)
    tool_items = collector.collect_tools(max_items=total)
    guide_items = collector.collect_guides(max_items=total)

    available = {
        Category.NEWS: len(news_items),
        Category.TOOL: len(tool_items),
        Category.GUIDE: len(guide_items),
    }

    print(f"  NEWS:  {available[Category.NEWS]} items found")
    print(f"  TOOL:  {available[Category.TOOL]} items found")
    print(f"  GUIDE: {available[Category.GUIDE]} items found")
    print()

    # -------------------------
    # Step 2: Calculate with fallback
    # -------------------------
    print("[Step 2] Calculating targets with fallback...")
    targets = calculate_targets_with_fallback(total, available)

    print(f"  NEWS:  {targets[Category.NEWS]} (target: {int(total * 0.4)})")
    print(f"  TOOL:  {targets[Category.TOOL]} (target: {int(total * 0.4)})")
    print(f"  GUIDE: {targets[Category.GUIDE]} (target: {total - int(total * 0.4) * 2})")
    print(f"  Total: {sum(targets.values())}")
    print()

    # -------------------------
    # Build final item list
    # -------------------------
    final_items: List[NewsItem] = []
    final_items.extend(news_items[:targets[Category.NEWS]])
    final_items.extend(tool_items[:targets[Category.TOOL]])
    final_items.extend(guide_items[:targets[Category.GUIDE]])

    # -------------------------
    # Dry-run: Show simulation and exit
    # -------------------------
    if dry_run:
        print("[DRY-RUN] Simulation Results:")
        print("-" * 60)
        for i, item in enumerate(final_items, 1):
            print(f"{i:2}. [{item.category.value:5}] {item.title[:50]}...")
            print(f"     Source: {item.source}")
            print(f"     URL: {item.url[:60]}...")
            print(f"     [DryRun] Would post to X: {item.title[:40]}...")
            print()
        print("-" * 60)
        print(f"Total: {len(final_items)} articles would be generated.")
        print("(No files written, no URLs logged, no X posts)")
        return 0

    # -------------------------
    # Step 3: Generate articles
    # -------------------------
    if not final_items:
        print("[!] No items to generate.")
        return 0

    generator = ArticleGenerator(api_key=api_key, model_name="gemini-3-flash-preview")
    image_handler = ImageHandler(api_key=api_key, model_name="gemini-3-flash-preview")
    out_dir = repo_root / "content" / "posts"
    static_dir = repo_root / "static"
    images_dir = static_dir / "images" / "posts"

    # Twitter Poster (optional)
    twitter_poster: Optional[TwitterPoster] = None
    twitter_enabled = is_twitter_configured() and TWEEPY_AVAILABLE
    if twitter_enabled:
        try:
            twitter_poster = TwitterPoster()
            print("[Twitter] API configured and ready.")
        except Exception as e:
            print(f"[Twitter] Disabled: {e}")
            twitter_poster = None
    else:
        if not TWEEPY_AVAILABLE:
            print("[Twitter] Disabled: tweepy not installed")
        else:
            print("[Twitter] Disabled: API credentials not configured")

    # test-one: 1ä»¶ã ã‘
    if test_one:
        final_items = final_items[:1]
        print("[TEST-ONE] Generating 1 article only...")
    else:
        print(f"[Step 3] Generating {len(final_items)} articles...")

    print()

    # ã‚µã‚¤ãƒˆã®ãƒ™ãƒ¼ã‚¹URL (ãƒ‡ãƒ—ãƒ­ã‚¤å¾Œã®URL)
    base_url = "https://ai.negi-lab.com"

    success_count = 0
    twitter_success = 0
    for idx, item in enumerate(final_items, 1):
        print(f"[{idx}/{len(final_items)}] Generating: {item.title[:40]}...")

        try:
            # Generate article
            title, body = generator.generate_article(item)

            # Prepare output
            now_jst = datetime.now(JST)
            date_midnight = now_jst.replace(hour=0, minute=0, second=0, microsecond=0)
            ymd = date_midnight.strftime("%Y-%m-%d")

            article_id = f"{ymd}-{uuid.uuid4().hex[:8]}"
            filename = f"{article_id}.md"
            out_path = out_dir / filename

            # Generate and save image locally
            print(f"  Generating image...")
            image_path = image_handler.generate_and_save_image(
                title=title,
                body=body,
                category=item.category,
                article_id=article_id,
                output_dir=images_dir,
            )

            # Determine tags
            tags = ["GenAI"]
            if item.category == Category.NEWS:
                tags.extend(["é€Ÿå ±", "AIãƒ‹ãƒ¥ãƒ¼ã‚¹"])
            elif item.category == Category.TOOL:
                tags.extend(["ãƒ„ãƒ¼ãƒ«", "ãƒ¬ãƒ“ãƒ¥ãƒ¼"])
            else:
                tags.extend(["ã‚¬ã‚¤ãƒ‰", "ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«"])

            # Write file
            write_hugo_markdown(
                out_path=out_path,
                title=title,
                date_jst=date_midnight,
                image_url=image_path,
                category=item.category,
                tags=tags,
                body=body,
            )

            # Mark as processed
            processed_store.add(item.url)
            processed_store.save()

            print(f"  âœ“ Saved: {filename}")
            success_count += 1

            # TwitteræŠ•ç¨¿
            if twitter_poster:
                # ãƒ­ãƒ¼ã‚«ãƒ«ç”»åƒã‚’ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦èª­ã¿è¾¼ã¿
                image_data = image_handler.download_image_to_bytes(image_path, static_dir)
                article_url = f"{base_url}/posts/{article_id}/"
                if twitter_poster.post_article(
                    title=title,
                    url=article_url,
                    image_data=image_data,
                    category=item.category,
                    hashtags=tags,
                ):
                    print(f"  âœ“ Posted to X")
                    twitter_success += 1
                else:
                    print(f"  âœ— X post failed")

            # Rate limit
            if idx < len(final_items):
                print(f"  (Sleeping {SLEEP_SECONDS_PER_ARTICLE}s for rate limit...)")
                time.sleep(SLEEP_SECONDS_PER_ARTICLE)

        except Exception as e:
            print(f"  âœ— Failed: {e}")
            continue

    print()
    print("=" * 60)
    print(f"Done. Generated {success_count}/{len(final_items)} articles.")
    if twitter_poster:
        print(f"      Posted to X: {twitter_success}/{success_count}")
    print("=" * 60)

    return 0 if success_count > 0 else 1


if __name__ == "__main__":
    raise SystemExit(main())
