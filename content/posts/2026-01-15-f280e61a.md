---
title: "LLMã‚’æœ€å°ãƒªã‚½ãƒ¼ã‚¹ã§æœ€é©åŒ–ã™ã‚‹ï¼šLoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Œå…¨ã‚¬ã‚¤ãƒ‰"
date: 2026-01-15T00:00:00+09:00
cover:
  image: "/images/posts/2026-01-15-f280e61a.png"
  alt: "AI generated thumbnail"
  relative: false
categories:
  - "AI Guide"
tags:
  - "GenAI"
  - "ã‚¬ã‚¤ãƒ‰"
  - "ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«"
---
## ã“ã®è¨˜äº‹ã§å­¦ã¹ã‚‹ã“ã¨

- å·¨å¤§ãªãƒ¢ãƒ‡ãƒ«ã‚’ä¸€èˆ¬å®¶åº­ç”¨GPUï¼ˆVRAM 12GBã€œï¼‰ã§å†å­¦ç¿’ã•ã›ã‚‹LoRAã®ä»•çµ„ã¿
- Hugging Face `peft` ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ç”¨ã„ãŸå®Ÿè·µçš„ãªå®Ÿè£…ã‚³ãƒ¼ãƒ‰
- å­¦ç¿’ã‚’å¤±æ•—ã•ã›ãªã„ãŸã‚ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é¸å®šã¨ãƒ¡ãƒ¢ãƒªç¯€ç´„è¡“

## å‰ææ¡ä»¶

- **GPUç’°å¢ƒ**: NVIDIAè£½GPUï¼ˆVRAM 12GBä»¥ä¸Šæ¨å¥¨ã€‚RTX 3060/4060 Ti 16GBç­‰ï¼‰
- **OS**: Linux (Ubuntuæ¨å¥¨) ã¾ãŸã¯ WSL2
- **Python**: 3.10ä»¥ä¸Š
- **ãƒ©ã‚¤ãƒ–ãƒ©ãƒª**: `transformers`, `peft`, `bitsandbytes`, `accelerate`

ã€Œã¨ã‚Šã‚ãˆãšå‹•ã‘ã°ã„ã„ã€ã¨ã„ã†è€ƒãˆã¯æ¨ã¦ã¦ãã ã•ã„ã€‚ãªãœãã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®šã™ã‚‹ã®ã‹ã€æŠ€è¡“è€…ãªã‚‰è£å´ã‚’æ„è­˜ã—ã¾ã—ã‚‡ã†ã€‚

## Step 1: ç’°å¢ƒæº–å‚™

ã¾ãšã¯ä¾å­˜ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§ã™ã€‚LoRAã‚’åŠ¹ç‡ã‚ˆãè¡Œã†ã«ã¯ã€ãƒ¢ãƒ‡ãƒ«ã‚’4-bité‡å­åŒ–ã—ã¦ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãŸã‚ã® `bitsandbytes` ãŒå¿…é ˆã§ã™ã€‚

```bash
# ä»®æƒ³ç’°å¢ƒã®ä½œæˆï¼ˆæ¨å¥¨ï¼‰
python -m venv venv
source venv/bin/activate

# å¿…é ˆãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install -U transformers peft accelerate bitsandbytes datasets
```

GPUãŒæ­£ã—ãèªè­˜ã•ã‚Œã¦ã„ã‚‹ã‹ã€ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ç¢ºèªã—ã¦ãã ã•ã„ã€‚
```bash
python -c "import torch; print(torch.cuda.is_available())"
```
ã“ã“ã§ `False` ãŒå‡ºã‚‹ã‚ˆã†ãªã‚‰ã€ç’°å¢ƒæ§‹ç¯‰ä»¥å‰ã®å•é¡Œã§ã™ã€‚ãƒ‰ãƒ©ã‚¤ãƒã‚’å…¥ã‚Œç›´ã—ã¦ãã ã•ã„ã€‚

## Step 2: åŸºæœ¬è¨­å®š

LoRAã®æœ¬è³ªã¯ã€å…ƒã®é‡ã¿ã‚’å›ºå®šã—ã€å°ã•ãªå·®åˆ†è¡Œåˆ—ï¼ˆAdapterï¼‰ã ã‘ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ã«ã‚ã‚Šã¾ã™ã€‚ã“ã®ã€Œå·®åˆ†ã€ã®ã‚µã‚¤ã‚ºã‚’æ±ºã‚ã‚‹ã®ãŒ `r` (rank) ã§ã™ã€‚

ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã¯ã€Llama-3ã‚„Mistralãªã©ã®ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ¢ãƒ‡ãƒ«ã‚’å¯¾è±¡ã¨ã—ãŸè¨­å®šä¾‹ã§ã™ã€‚

```python
from peft import LoraConfig, get_peft_model
from transformers import AutoModelForCausalLM, BitsAndBytesConfig
import torch

model_id = "meta-llama/Meta-Llama-3-8B" # ã¾ãŸã¯ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹

# 1. ãƒ¡ãƒ¢ãƒªç¯€ç´„ã®ãŸã‚ã®4-bité‡å­åŒ–è¨­å®š
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# 2. ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map="auto"
)

# 3. LoRAè¨­å®š
lora_config = LoraConfig(
    r=16,                         # ãƒ©ãƒ³ã‚¯æ•°ã€‚8ã€œ32ãŒä¸€èˆ¬çš„ã€‚ä¸Šã’ã™ãã‚‹ã¨ãƒ¡ãƒ¢ãƒªã‚’é£Ÿã†
    lora_alpha=32,                # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°ã€‚é€šå¸¸ r ã®2å€ã«è¨­å®š
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"], # ã©ã®å±¤ã‚’å­¦ç¿’ã•ã›ã‚‹ã‹
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

# 4. ãƒ¢ãƒ‡ãƒ«ã«LoRAã‚’é©ç”¨
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
```

`target_modules` ã¯ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ã‚ˆã£ã¦ç•°ãªã‚Šã¾ã™ã€‚`model` ã‚’ print ã—ã¦ã€ç·šå½¢å±¤ã®åå‰ã‚’ç¢ºèªã™ã‚‹ç™–ã‚’ã¤ã‘ã¦ãã ã•ã„ã€‚

## Step 3: å®Ÿè¡Œã¨ç¢ºèª

å­¦ç¿’ã«ã¯ Hugging Face ã® `SFTTrainer` (trlãƒ©ã‚¤ãƒ–ãƒ©ãƒª) ã‚’ä½¿ã†ã®ãŒç¾ä»£ã®å®šçŸ³ã§ã™ã€‚

```python
from trl import SFTTrainer
from transformers import TrainingArguments
from datasets import load_dataset

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™ï¼ˆjsonlå½¢å¼ã‚’æƒ³å®šï¼‰
# dataset = load_dataset("json", data_files="train_data.jsonl", split="train")

training_args = TrainingArguments(
    output_dir="./lora-llama3-results",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    logging_steps=10,
    num_train_epochs=3,
    save_strategy="epoch",
    bf16=True, # GPUãŒå¯¾å¿œã—ã¦ã„ã‚Œã°å¿…ãšTrueã«
    optim="paged_adamw_32bit"
)

trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    dataset_text_field="text",
    max_seq_length=1024,
    args=training_args,
)

trainer.train()

# ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ã¿ä¿å­˜
model.save_pretrained("./final_lora_adapter")
```

å­¦ç¿’ãŒçµ‚ã‚ã‚‹ã¨ã€`adapter_model.bin` ã¨ã„ã†éå¸¸ã«å°ã•ãªãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¾ã™ã€‚ã“ã‚ŒãŒã‚ãªãŸã®æˆæœç‰©ã§ã™ã€‚

## ã‚ˆãã‚ã‚‹ã‚¨ãƒ©ãƒ¼ã¨å¯¾å‡¦æ³•

### ã‚¨ãƒ©ãƒ¼1: CUDA Out of Memory (OOM)

```
torch.cuda.OutOfMemoryError: CUDA out of memory.
Tried to allocate 256.00 MiB (GPU 0; 12.00 GiB total capacity; ...)
```

**è§£æ±ºç­–:**
- `per_device_train_batch_size` ã‚’ 1 ã«ä¸‹ã’ã€`gradient_accumulation_steps` ã‚’ä¸Šã’ã¦ãã ã•ã„ã€‚
- `max_seq_length` ã‚’çŸ­ãè¨­å®šï¼ˆ512ãªã©ï¼‰ã—ã¦ãã ã•ã„ã€‚
- ãã‚Œã§ã‚‚ãƒ€ãƒ¡ãªã‚‰ã€ã‚‚ã£ã¨é«˜ã„GPUã‚’è²·ã†ã‹ã€ã‚¯ãƒ©ã‚¦ãƒ‰ï¼ˆPaperspaceã‚„Colabï¼‰ã«é‡‘ã‚’æ‰•ã£ã¦ãã ã•ã„ã€‚

### ã‚¨ãƒ©ãƒ¼2: ValueError: Target modules not found

```
ValueError: Target modules {'q_proj'} not found in the base model.
```

**è§£æ±ºç­–:**
ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦å±¤ã®åå‰ãŒé•ã„ã¾ã™ã€‚Mistralç³»ã¯ `q_proj`, `v_proj` ã§ã™ãŒã€ä»–ã®ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ã§ã¯ `query_key_value` ãªã©ã«ãªã£ã¦ã„ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚`print(model)` ã§æ§‹é€ ã‚’å©ãå‡ºã—ã¦ãã ã•ã„ã€‚

## ã¾ã¨ã‚ã¨æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

LoRAã¯é­”æ³•ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚é©åˆ‡ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é¸å®šãŒã‚ã£ã¦åˆã‚ã¦æ©Ÿèƒ½ã—ã¾ã™ã€‚

1. **æ¬¡ã¯æ¨è«–ã‚’è©¦ã™**: å­¦ç¿’ã—ãŸã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã« `merge_and_unload()` ã—ã¦çµ±åˆã—ã€æ¨è«–é€Ÿåº¦ã‚’æœ€é©åŒ–ã—ã¾ã—ã‚‡ã†ã€‚
2. **è©•ä¾¡æŒ‡æ¨™ã‚’å…¥ã‚Œã‚‹**: LossãŒä¸‹ãŒã£ã¦ã„ã‚‹ã‹ã‚‰ã¨ã„ã£ã¦ã€è³¢ããªã£ã¦ã„ã‚‹ã¨ã¯é™ã‚Šã¾ã›ã‚“ã€‚ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚»ãƒƒãƒˆã‚’ç”¨æ„ã—ã¾ã—ã‚‡ã†ã€‚

ã€Œå‹•ã„ãŸã€ã ã‘ã§æº€è¶³ã™ã‚‹ã®ã¯ç´ äººã§ã™ã€‚ãªãœãã®è¨­å®šã§ç²¾åº¦ãŒå‡ºãŸã®ã‹ã€ãƒ­ã‚°ã‚’åŸ·ç­†ã™ã‚‹ã¾ã§ãŒç ”ç©¶å“¡ã®ä»•äº‹ã§ã™ã€‚

---

---

## ğŸ“š ã•ã‚‰ã«å­¦ç¿’ã‚’æ·±ã‚ã‚‹ãŸã‚ã®ãƒªã‚½ãƒ¼ã‚¹

ã“ã®è¨˜äº‹ã®å†…å®¹ã‚’ã‚ˆã‚Šæ·±ãç†è§£ã™ã‚‹ãŸã‚ã«ã€ä»¥ä¸‹ã®æ›¸ç±ãƒ»æ•™æãŒãŠã™ã™ã‚ã§ã™ï¼š

- **[NVIDIA RTX 4070 SUPER](https://www.amazon.co.jp/s?k=RTX%204070%20SUPER%20%E3%82%B0%E3%83%A9%E3%83%95%E3%82%A3%E3%83%83%E3%82%AF%E3%83%9C%E3%83%BC%E3%83%89&tag=negi3939-22)** - ãƒ­ãƒ¼ã‚«ãƒ«LLMã«æœ€é©ãª12GB VRAM
- **[NVIDIA RTX 4090](https://www.amazon.co.jp/s?k=RTX%204090%20%E3%82%B0%E3%83%A9%E3%83%95%E3%82%A3%E3%83%83%E3%82%AF%E3%83%9C%E3%83%BC%E3%83%89&tag=negi3939-22)** - æœ€é«˜æ€§èƒ½24GB VRAMã€å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«å‘ã‘
- **[å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€](https://www.amazon.co.jp/s?k=%E5%A4%A7%E8%A6%8F%E6%A8%A1%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E5%85%A5%E9%96%80%20%E6%9B%B8%E7%B1%8D&tag=negi3939-22)** - LLMã®åŸºç¤ã‹ã‚‰å®Ÿè£…ã¾ã§
- **[ã‚²ãƒ¼ãƒŸãƒ³ã‚°PC](https://www.amazon.co.jp/s?k=%E3%82%B2%E3%83%BC%E3%83%9F%E3%83%B3%E3%82%B0PC%20RTX4070%20%E3%83%A1%E3%83%A2%E3%83%AA32GB&tag=negi3939-22)** - ãƒ­ãƒ¼ã‚«ãƒ«LLMå®Ÿè¡Œã«æœ€é©ãªã‚¹ãƒšãƒƒã‚¯


<div style="display: flex; gap: 10px; flex-wrap: wrap; margin: 15px 0;">
<a href="https://www.amazon.co.jp/s?k=NVIDIA%20RTX%204090&tag=negi3939-22" target="_blank" rel="noopener sponsored" style="padding: 10px 20px; background: linear-gradient(135deg, #ff9900, #ff6600); color: white; text-decoration: none; border-radius: 6px; font-weight: bold;">ğŸ” Amazonã§ã€ŒNVIDIA RTX 4090ã€ã‚’æ¤œç´¢</a>
<a href="https://hb.afl.rakuten.co.jp/hgc/5000cbfd.5f52567b.5000cbff.924460a4/?pc=https%3A%2F%2Fsearch.rakuten.co.jp%2Fsearch%2Fmall%2FNVIDIA%2520RTX%25204090%2F&m=https%3A%2F%2Fsearch.rakuten.co.jp%2Fsearch%2Fmall%2FNVIDIA%2520RTX%25204090%2F" target="_blank" rel="noopener sponsored" style="padding: 10px 20px; background: linear-gradient(135deg, #bf0000, #8b0000); color: white; text-decoration: none; border-radius: 6px; font-weight: bold;">ğŸ” æ¥½å¤©ã§æ¤œç´¢</a>
</div>

<small style="color: #888;">â€»ä¸Šè¨˜ãƒªãƒ³ã‚¯ã¯ã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆãƒªãƒ³ã‚¯ã§ã™ã€‚</small>
