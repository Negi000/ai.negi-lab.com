---
title: "LLMを安価に自分専用にする「LoRAファインチューニング」実践ガイド"
date: 2026-01-14T00:00:00+09:00
cover:
  image: "/images/posts/2026-01-14-95f64438.png"
  alt: "AI generated thumbnail"
  relative: false
categories:
  - "AI Guide"
tags:
  - "GenAI"
  - "ガイド"
  - "チュートリアル"
---
## この記事で学べること

- 巨大なモデルを一般家庭用GPU（VRAM 16GB〜）で学習させるための具体的な技術構成
- Hugging Faceの`peft`ライブラリを用いた、LoRAパラメータの最適な設定値
- 学習を失敗させないためのデータ準備と、エラー回避のトラブルシューティング

いいか、いまだに「フルファインチューニングにA100が数枚必要だ」なんて嘆いているのは情弱だけだ。LoRA（Low-Rank Adaptation）を使えば、モデルの大部分をフリーズさせたまま、重みの差分だけを効率よく学習できる。この記事では、現場で即戦力になる「QLoRA（4-bit量子化LoRA）」をベースに解説する。

## 前提条件

- **GPU:** NVIDIA製 12GB以上のVRAM（16GB以上を推奨。RTX 3060/4060 Ti以上が目安だ）
- **OS:** Linux（Ubuntu推奨）またはWSL2。Windowsネイティブでの環境構築は時間の無駄だ、やめておけ。
- **Python:** 3.10以上
- **Hugging Faceのアカウントとトークン:** モデルのロードに必須。

## Step 1: 環境準備

まずはライブラリだ。適当なバージョンを入れると依存関係で死ぬ。以下のコマンドで、最新かつ安定しているスタックを叩き込め。

```bash
# 仮想環境の作成（必須だ、システムを汚すな）
python -m venv venv
source venv/bin/activate

# 必要なライブラリのインストール
pip install -U pip
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
pip install transformers datasets accelerate peft bitsandbytes bitsandbytes
```

特に`bitsandbytes`は4-bit量子化に、`peft`はLoRA本体に必要だ。これがないと始まらない。

## Step 2: 基本設定

学習の肝となるのは`LoraConfig`だ。ここでの設定をミスると、学習が進まないか、あるいはモデルが崩壊する。以下のスクリプトをテンプレートとして使え。

```python
from peft import LoraConfig, get_peft_model, TaskType
from transformers import AutoModelForCausalLM, BitsAndBytesConfig
import torch

# 1. モデルを4-bitでロード（メモリ節約の要）
model_id = "meta-llama/Llama-2-7b-hf"  # または任意のモデル
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)

# 2. LoRA設定
# r（ランク）は8〜16がコスパ最強。128とかにしても精度は大して変わらず重くなるだけだ。
config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"], # モデルによって変える必要がある
    lora_dropout=0.05,
    bias="none",
    task_type=TaskType.CAUSAL_LM
)

# 3. LoRAモデルの適用
model = get_peft_model(model, config)
model.print_trainable_parameters()
```

## Step 3: 実行と確認

学習コードの全体を書くと長くなるが、基本的には`transformers.Trainer`か、より簡便な`trl.SFTTrainer`を使え。初心者はデータのフォーマットでミスをする。データセットは必ず`{"instruction": "...", "input": "...", "output": "..."}`のような形式に整えておけ。

学習開始コマンドの例：
```python
from trl import SFTTrainer
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./lora-output",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-4, # LoRAならこの程度が妥当
    num_train_epochs=3,
    logging_steps=10,
    fp16=True, # 16-bit混合精度
    save_strategy="steps",
    save_steps=100,
)

trainer = SFTTrainer(
    model=model,
    train_dataset=dataset, # 事前にロードしたdatasetsオブジェクト
    dataset_text_field="text",
    max_seq_length=512,
    args=training_args,
)

trainer.train()
```

学習が終わると、`output_dir`に数MB程度の`adapter_model.bin`が生成される。これが君の努力の結晶だ。ベースモデルそのものを書き換えたわけではないので注意しろ。

## よくあるエラーと対処法

### エラー1: CUDA Out of Memory (OOM)

```text
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate ...
```

**解決策:**
1. `per_device_train_batch_size`を1に下げろ。その分`gradient_accumulation_steps`を増やして実質的なバッチサイズを維持するんだ。
2. `max_seq_length`を短くしろ。512でも足りないなら256だ。
3. `bnb_4bit_use_double_quant=True`になっているか再確認しろ。

### エラー2: 学習が進まない（Lossが変化しない）

**解決策:**
学習率（learning_rate）が高すぎるか低すぎる。2e-4から5e-5の範囲で調整してみろ。また、`target_modules`に`q_proj`や`v_proj`だけでなく、`k_proj`や`o_proj`を追加すると改善することがある。ただし、メモリ使用量とトレードオフだ。

## まとめと次のステップ

LoRAをマスターすれば、限られたリソースでも特定のドメイン（業界用語や社内ルールなど）に特化した最強のLLMを錬成できる。

次のステップとしては以下を勧める：
1. **Merge（マージ）:** 学習したアダプタをベースモデルに統合して、推論速度を上げる。
2. **Evaluation（評価）:** 学習したモデルが「壊れていないか」を確認するためのベンチマーク（LM Evaluation Harnessなど）を回す。
3. **RLHF（人間からのフィードバック）:** 精度に不満があるなら、DPO（Direct Preference Optimization）などの手法にステップアップしろ。

以上だ。理屈はいいから、まずは手を動かせ。

---


---

## 関連商品をチェック

<div style="display: flex; gap: 12px; flex-wrap: wrap; margin: 20px 0;">
  <a href="https://www.amazon.co.jp/s?k=RTX%204090%2024GB&tag=negi3939-22" target="_blank" rel="noopener noreferrer sponsored" style="display: inline-flex; align-items: center; gap: 8px; padding: 12px 24px; background: linear-gradient(135deg, #ff9900 0%, #ff6600 100%); color: white; text-decoration: none; border-radius: 8px; font-weight: bold; box-shadow: 0 4px 12px rgba(255, 153, 0, 0.3); transition: transform 0.2s;">
    <svg width="20" height="20" viewBox="0 0 24 24" fill="currentColor"><path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm-2 15l-5-5 1.41-1.41L10 14.17l7.59-7.59L19 8l-9 9z"/></svg>
    Amazonで「RTX 4090 24GB」を検索
  </a>
  <a href="https://search.rakuten.co.jp/search/mall/RTX%204090%2024GB/?scid=5000cbfd.5f52567b.5000cbff.924460a4" target="_blank" rel="noopener noreferrer sponsored" style="display: inline-flex; align-items: center; gap: 8px; padding: 12px 24px; background: linear-gradient(135deg, #bf0000 0%, #8b0000 100%); color: white; text-decoration: none; border-radius: 8px; font-weight: bold; box-shadow: 0 4px 12px rgba(191, 0, 0, 0.3); transition: transform 0.2s;">
    <svg width="20" height="20" viewBox="0 0 24 24" fill="currentColor"><path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm-2 15l-5-5 1.41-1.41L10 14.17l7.59-7.59L19 8l-9 9z"/></svg>
    楽天で「RTX 4090 24GB」を検索
  </a>
</div>

<small style="color: #888;">※上記リンクはアフィリエイトリンクです。購入により当サイトに収益が発生する場合があります。</small>
