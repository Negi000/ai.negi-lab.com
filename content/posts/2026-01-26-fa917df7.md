---
title: "NVIDIA DGX Sparkを手に入れたら最初にやるべきローカルLLM環境構築ガイド"
date: 2026-01-26T00:00:00+09:00
description: "NVIDIA DGX Spark（Jetson Orin搭載機）の初期セットアップ。メモリ制限を回避するためのスワップ領域の最適化"
cover:
  image: "/images/posts/2026-01-26-fa917df7.png"
  alt: "AI generated thumbnail"
  relative: false
categories:
  - "AI Guide"
tags:
  - "GenAI"
  - "ガイド"
  - "チュートリアル"
---
## この記事で学べること

- NVIDIA DGX Spark（Jetson Orin搭載機）の初期セットアップ
- メモリ制限を回避するためのスワップ領域の最適化
- ローカルLLMを高速に動かすためのllama.cppの導入と実行

## 前提条件

- NVIDIA DGX Spark本体（Jetson Orin Nano/NXベース）
- Ubuntu 20.04以降がインストール済みの環境
- インターネット接続環境
- ターミナル操作の基本的な知識

## Step 1: 環境準備

まずはOSのパッケージを最新の状態にし、LLMのビルドに必要なツールをインストールします。SIer時代、この「最初のアプデ」を怠って後でライブラリの依存関係に泣かされたことが何度もありました。みなさんも、ここは横着せずに進めましょう。

また、DGX Sparkのようなエッジデバイスはメモリが限られているため、スワップ領域（仮想メモリ）を確保しておくのが「お約束」です。

```bash
# パッケージの更新
sudo apt update && sudo apt upgrade -y

# ビルドツールのインストール
sudo apt install -y build-essential git cmake libcurl4-openssl-dev

# スワップファイルの作成（8GB分）
# メモリ不足でビルドが落ちるのを防ぎます
sudo fallocate -l 8G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile

# 再起動後も有効にする設定
echo '/swapfile none swap sw 0 0' | sudo tee -a /etc/fstab
```

## Step 2: 基本設定

次に、NVIDIAのGPU（CUDA）を最大限活かすために、LLM実行ライブラリ「llama.cpp」をビルドします。Jetson向けに最適化をかけるのがポイントです。

個人的には、Dockerを使うのも手ですが、最初はベタにビルドしたほうがトラブルの切り分けがしやすくて好きですね。

```bash
# llama.cppのリポジトリをクローン
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp

# CUDAを有効にしてビルド
# DGX SparkのGPU性能を引き出すために必須の設定です
mkdir build
cd build
cmake .. -DGGML_CUDA=ON
cmake --build . --config Release -j$(nproc)
```

次に、Python環境で動かすための設定ファイル（依存ライブラリ）をインストールしておきます。

```bash
# 必要なライブラリのインストール
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip3 install numpy sentencepiece
```

## Step 3: 実行と確認

いよいよモデルを動かしてみましょう。DGX Sparkで快適に動かすなら、Llama-3やMistralの「4ビット量子化（GGUF形式）」がおすすめです。

以下のPythonスクリプトは、ビルドしたバイナリを介して推論を行う簡単な例です。

```python
import subprocess

def run_llm(prompt):
    # 量子化されたモデルファイルを指定（あらかじめダウンロードしておく必要があります）
    model_path = "./models/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf"

    cmd = [
        "./bin/llama-cli",
        "-m", model_path,
        "-p", prompt,
        "-n", "128",          # 生成するトークン数
        "--n-gpu-layers", "99" # すべてのレイヤーをGPUに乗せる
    ]

    try:
        result = subprocess.run(cmd, capture_output=True, text=True)
        print("回答:", result.stdout)
    except Exception as e:
        print(f"エラーが発生しました: {e}")

if __name__ == "__main__":
    my_prompt = "AIエンジニアとして成功する秘訣を教えてください。"
    run_llm(my_prompt)
```

## よくあるエラーと対処法

### エラー1: CUDAのパスが通っていない

```
CMake Error: CUDA_TOOLKIT_ROOT_DIR not found
```

**解決策:** JetsonではCUDAが `/usr/local/cuda` にインストールされていることが多いです。パスを通してから再度ビルドしてください。

```bash
export PATH=/usr/local/cuda/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}
source ~/.bashrc
```

### エラー2: Out of Memory (OOM) でプロセスが落ちる

```
Killed (core dumped)
```

**解決策:** モデルが大きすぎるか、GPUレイヤーの指定が多すぎます。`--n-gpu-layers` の値を少しずつ下げるか、より小さいパラメータ（3Bモデルなど）を試してみてください。正直、DGX Sparkなら8Bクラスが限界値に近いので、欲張りすぎないのがコツです。

## まとめ

ハッカソンでNVIDIA DGX Sparkを手に入れるなんて、本当に羨ましい限りです。
最初は「こんな小さい箱で何ができるの？」と思うかもしれませんが、最適化次第で驚くほどキビキビとLLMが動きます。

今回紹介したスワップ領域の確保と、CUDA有効化のビルドさえクリアすれば、あとは自分専用の「オフラインAIアシスタント」を作るのも夢ではありません。

いかがでしたか？ ぜひ手元のマシンに火を灯して、AI開発の楽しさを体感してみてくださいね。

---

## 📚 さらに学習を深めるためのリソース

この記事の内容をより深く理解するために、以下の書籍・教材がおすすめです：

- **[NVIDIA RTX 4070 SUPER](https://www.amazon.co.jp/s?k=RTX%204070%20SUPER%20%E3%82%B0%E3%83%A9%E3%83%95%E3%82%A3%E3%83%83%E3%82%AF%E3%83%9C%E3%83%BC%E3%83%89&tag=negi3939-22)** - ローカルLLMに最適な12GB VRAM
- **[NVIDIA RTX 4090](https://www.amazon.co.jp/s?k=RTX%204090%20%E3%82%B0%E3%83%A9%E3%83%95%E3%82%A3%E3%83%83%E3%82%AF%E3%83%9C%E3%83%BC%E3%83%89&tag=negi3939-22)** - 最高性能24GB VRAM、大規模モデル向け
- **[大規模言語モデル入門](https://www.amazon.co.jp/s?k=%E5%A4%A7%E8%A6%8F%E6%A8%A1%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E5%85%A5%E9%96%80%20%E6%9B%B8%E7%B1%8D&tag=negi3939-22)** - LLMの基礎から実装まで
- **[ゲーミングPC](https://www.amazon.co.jp/s?k=%E3%82%B2%E3%83%BC%E3%83%9F%E3%83%B3%E3%82%B0PC%20RTX4070%20%E3%83%A1%E3%83%A2%E3%83%AA32GB&tag=negi3939-22)** - ローカルLLM実行に最適なスペック


<div style="display: flex; gap: 10px; flex-wrap: wrap; margin: 15px 0;">
<a href="https://www.amazon.co.jp/s?k=NVIDIA%20Jetson%20%E9%96%8B%E7%99%BA%E8%80%85%E3%82%AD%E3%83%83%E3%83%88%20AI%E3%82%A8%E3%83%83%E3%82%B8&tag=negi3939-22" target="_blank" rel="noopener sponsored" style="padding: 10px 20px; background: linear-gradient(135deg, #ff9900, #ff6600); color: white; text-decoration: none; border-radius: 6px; font-weight: bold;">🔍 Amazonで「NVIDIA Jetson 開発者キット AIエッジ」を検索</a>
<a href="https://search.rakuten.co.jp/search/mall/NVIDIA%20Jetson%20%E9%96%8B%E7%99%BA%E8%80%85%E3%82%AD%E3%83%83%E3%83%88%20AI%E3%82%A8%E3%83%83%E3%82%B8/?scid=5000cbfd.5f52567b.5000cbff.924460a4" target="_blank" rel="noopener sponsored" style="padding: 10px 20px; background: linear-gradient(135deg, #bf0000, #8b0000); color: white; text-decoration: none; border-radius: 6px; font-weight: bold;">🔍 楽天で検索</a>
</div>

<small style="color: #888;">※上記リンクはアフィリエイトリンクです。</small>
