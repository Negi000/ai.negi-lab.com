---
title: "2018年の型落ちPCで16BのAIをサクサク動かす方法：高価なGPUなしでローカルLLMを楽しむための完全ガイド"
date: 2026-02-06T00:00:00+09:00
description: "高価なNVIDIA製GPUがなくても、古いCPUだけで大規模言語モデル（LLM）を動かす具体的な手順。「Mixture of Experts (MoE)」..."
cover:
  image: "https://image.pollinations.ai/prompt/Minimalist%20isometric%203D%20illustration%20of%20a%20computer%20processor%20chip%20with%20glowing%20geometric%20neural%20network%20pathways%2C%20clean%20vector%20art%20style%2C%20dark%20professional%20background%20with%20cyan%20and%20electric%20blue%20light%20trails%2C%20symbolic%20representation%20of%20data%20flow%20and%20hardware%20optimization%2C%20high-tech%20educational%20aesthetic.?width=1200&height=630&nologo=true"
  alt: "AI generated thumbnail"
  relative: false
categories:
  - "AI Guide"
tags:
  - "GenAI"
  - "ガイド"
  - "チュートリアル"
---
## この記事で学べること

- 高価なNVIDIA製GPUがなくても、古いCPUだけで大規模言語モデル（LLM）を動かす具体的な手順
- 「Mixture of Experts (MoE)」モデルがなぜ低スペックPCの救世主となるのかという技術的背景
- llama.cppを活用した、メモリ効率を最大化するためのビルドと設定の最適化手法
- 実際に10 TPS（1秒間に10トークン）という実用的な速度を出すためのチューニングのコツ

## 前提条件

- OS: Linux (Ubuntu 22.04以降を推奨) または Windows (WSL2)
- CPU: 第8世代Intel Core i3以上（AVX2命令セットをサポートしていること）
- RAM: 16GB以上（DDR4を推奨）
- ストレージ: SSD（モデルファイルの読み込み速度に直結します）

## なぜこの知識が重要なのか

みなさんも経験ありませんか？「ローカルLLMに興味はあるけれど、NVIDIAのRTX 4090なんて高くて買えないよ」と諦めてしまったこと。あるいは「自分のノートPCは数年前の古いモデルだから、AIなんて動くはずがない」と思い込んでいませんか？

実は、昨今のAI技術の進化は、高性能なハードウェアを追い求める方向だけでなく、いかに効率よく動かすかという方向にも凄まじい勢いで進んでいます。特に、Redditの「r/LocalLLaMA」コミュニティで話題になった「2018年のi3搭載ポテトPC（低スペックPCの俗称）で16Bモデルを動かす」という報告は、多くのユーザーに衝撃を与えました。

私自身、元SIerとして現場で古いサーバーの保守をしていた経験がありますが、当時は「最新のソフトウェアは最新のハードウェアで動かすもの」という固定観念がありました。しかし、ローカルLLMの世界は違います。量子化技術やMoE（Mixture of Experts）というアーキテクチャの登場により、数年前のオフィス用PCでも、十分に賢いAIを動かせる時代が来ているのです。

この記事では、単に「動いた」という報告に留まらず、なぜそれが可能なのか、そしてどうすれば皆さんの手元にある「眠っているPC」を最新のAI端末に変えられるのかを詳しく解説します。

## Step 1: 環境準備

まずは、CPUでLLMを動かすための最も標準的かつ強力なツールである「llama.cpp」を導入します。これはC++で書かれており、依存関係が非常に少なく、CPU実行に最適化されています。

以下のコマンドをターミナルで実行して、ビルド環境を整えましょう。

```bash
# 必要なビルドツールのインストール
sudo apt update
sudo apt install build-essential git cmake -y

# llama.cppのクローン
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp

# CPU（AVX2）向けに最適化してビルド
# 2018年以降のCPUであればAVX2が使えるため、標準的なビルドで十分高速です
mkdir build
cd build
cmake ..
cmake --build . --config Release -j$(nproc)
```

ビルドが完了すると、`bin`ディレクトリの中に`llama-cli`という実行ファイルが作成されます。これが私たちの武器になります。

ここで「なぜGPUを使わないのに高速なのか？」という疑問が湧くかもしれません。実は、第8世代以降のIntel CPUには、行列演算を高速化する命令セット（AVX2）が備わっています。llama.cppはこれを最大限に活用するように設計されているため、たとえi3であっても、計算のやり方次第では驚くほどのパフォーマンスを発揮するのです。

## Step 2: 基本設定とモデルの選定

次に、最も重要な「モデルの選定」に入ります。今回の肝は「16B（160億パラメータ）」という中規模のモデルを、どうやってメモリ16GBのPCに収めるかです。

ここで登場するのが「Mixture of Experts (MoE)」という仕組みです。16Bのモデルであっても、推論時にすべてのパラメータを使うわけではありません。特定の入力に対して、一部の「エキスパート（専門家）」となるパラメータだけを呼び出すため、計算量は2B（20億）クラスのモデルと同等に抑えられます。

おすすめは「DeepSeek-V2-Lite-Chat」のGGUF形式です。これを「Q4_K_M」という精度で量子化されたものを使います。

```bash
# モデルをダウンロードするためのツールを導入
pip install huggingface_hub

# モデルファイルをダウンロード（約10GB程度）
# 今回はBartowski氏が公開している量子化済みモデルを利用させていただきます
huggingface-cli download bartowski/DeepSeek-V2-Lite-Chat-GGUF --include "DeepSeek-V2-Lite-Chat-Q4_K_M.gguf" --local-dir ./models
```

ここで「Q4_K_M」というキーワードが出てきましたが、これはモデルの重みを4ビットに圧縮（量子化）することを意味します。本来16Bのモデルは32GB以上のメモリを必要としますが、4ビット量子化することで10GB程度までダイエットさせることが可能です。これにより、16GBのメモリを積んだ一般的なPCでも余裕を持って動作させることができるようになります。

## Step 3: 実行と確認

いよいよ実行です。CPUで動かす際に最も重要なのは「スレッド数（-t）」の指定です。

物理コア数と同じ、あるいはそれより少し少ない値を設定するのがベストプラクティスです。今回のようなi3（2コア4スレッドや4コア4スレッド）の場合は、「-t 4」または「-t 2」を試して、より速い方を採用します。

以下のコマンドで対話モードを起動してみましょう。

```bash
./bin/llama-cli \
  -m ./models/DeepSeek-V2-Lite-Chat-Q4_K_M.gguf \
  -n 512 \
  -t 4 \
  --repeat_penalty 1.1 \
  --color \
  -i \
  -p "User: こんにちは、自己紹介してください。\nAI:"
```

設定項目の意味を少し解説します。
- `-m`: モデルファイルのパス
- `-n 512`: 生成する最大トークン数
- `-t 4`: 使用するCPUスレッド数
- `-i`: インタラクティブモード（対話形式）

実行すると、古いPCとは思えないスピードで文字が生成されるはずです。Redditの報告にあるように、MoEモデルであれば、CPUのみの環境でも10 TPS（人間が読む速度より速い）に近いパフォーマンスが出ることも珍しくありません。

正直なところ、私も初めて型落ちPCでこれが動いたときは「今まで高いGPUを買ってきたのは何だったんだ……」と少し複雑な気持ちになりました。それほどまでにMoEとllama.cppの組み合わせは強力です。

## Step 4: 応用テクニック

さらに快適に使うためのテクニックを紹介します。

### メモリの「スワップ」を抑制する
Linuxの場合、メモリが足りなくなるとディスク（SSD/HDD）をメモリ代わりに使う「スワップ」が発生します。これが起きると速度が極端に低下します。
以下の設定で、極力物理メモリを使うように調整できます。

```bash
# スワップの優先度を下げる（一時的な変更）
sudo sysctl vm.swappiness=10
```

### コンテキストサイズを調整する
デフォルトではコンテキストサイズ（過去の会話を覚えている量）が大きく設定されていることがあり、これがメモリを圧迫します。メモリが厳しい場合は、`-c 2048`のように明示的に小さく指定することで、動作を安定させることができます。

```bash
# コンテキストサイズを2048に制限して実行
./bin/llama-cli -m ./models/DeepSeek-V2-Lite-Chat-Q4_K_M.gguf -c 2048 -t 4
```

## よくあるエラーと対処法

### エラー1: Illegal instruction (core dumped)

```
Illegal instruction (core dumped)
```

**原因:** CPUが古すぎて、llama.cppが要求する命令セット（AVX2など）に対応していない。
**解決策:** CMakeの実行時に、AVX2を無効にしてビルドし直す必要があります。ただし、速度は大幅に低下します。
```bash
cmake -DGGML_AVX2=OFF ..
cmake --build . --config Release
```

### エラー2: ggml_graph_compute: not enough memory

```
ggml_graph_compute: not enough memory
```

**原因:** 指定したモデルを読み込むためのメモリ（RAM）が不足しています。
**解決策:**
1. より小さい量子化サイズ（Q2_Kなど）のモデルを使用する。
2. ブラウザなどの重いアプリケーションを閉じて、空きメモリを確保する。
3. コンテキストサイズ（-c）の値を小さくする。

## ベストプラクティス

1. OSはLinuxを使うべし: Windowsのネイティブ環境よりも、Linux（またはWSL2）の方がCPUのメモリ管理効率が高く、推論速度が10〜20%向上する傾向にあります。
2. デュアルチャネルメモリを確認: メモリを1枚差し（16GB×1）で使っているなら、2枚差し（8GB×2）にするだけで帯域幅が倍になり、CPU推論速度が劇的に改善します。
3. モデルの「K-Quants」を選ぶ: GGUF形式にはQ4_0やQ4_1など種類がありますが、現在は「Q4_K_M」や「Q5_K_M」といったK-Quants系が、精度と速度のバランスが最も優れています。

## まとめ

いかがでしたか？「最新のAIは、最新のハードウェアを持つ選ばれた人たちだけのもの」という時代は、もう終わりを告げようとしています。

今回ご紹介した手法を使えば、2018年頃の「普通の事務用PC」が、驚くほど賢いAIアシスタントに生まれ変わります。個人的には、この「枯れた技術の水平思考」のようなアプローチが大好きです。高価な機材を揃える前に、まずは手元にある機材の限界まで引き出してみる。それこそが、エンジニアリングの醍醐味ではないでしょうか。

16Bという規模のモデルがローカルで、しかも古いCPUで実用的に動く。この事実は、プライバシーの観点からも、コストの観点からも、私たちのAIライフを大きく変えてくれるはずです。

「自分のPCじゃ無理だろうな」と決めつける前に、ぜひ一度 llama.cpp をビルドして、DeepSeek-V2-LiteのようなMoEモデルを試してみてください。きっと、ターミナルに流れる文字の速さに驚くはずですよ。

それでは、素晴らしいAIライフを！ねぎでした。

---

## 📚 さらに学習を深めるためのリソース

この記事の内容をより深く理解するために、以下の書籍・教材がおすすめです：

- **[NVIDIA RTX 4070 SUPER](https://www.amazon.co.jp/s?k=RTX%204070%20SUPER%20%E3%82%B0%E3%83%A9%E3%83%95%E3%82%A3%E3%83%83%E3%82%AF%E3%83%9C%E3%83%BC%E3%83%89&tag=negi3939-22)** - ローカルLLMに最適な12GB VRAM
- **[NVIDIA RTX 4090](https://www.amazon.co.jp/s?k=RTX%204090%20%E3%82%B0%E3%83%A9%E3%83%95%E3%82%A3%E3%83%83%E3%82%AF%E3%83%9C%E3%83%BC%E3%83%89&tag=negi3939-22)** - 最高性能24GB VRAM、大規模モデル向け
- **[大規模言語モデル入門](https://www.amazon.co.jp/s?k=%E5%A4%A7%E8%A6%8F%E6%A8%A1%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E5%85%A5%E9%96%80%20%E6%9B%B8%E7%B1%8D&tag=negi3939-22)** - LLMの基礎から実装まで
- **[ゲーミングPC](https://www.amazon.co.jp/s?k=%E3%82%B2%E3%83%BC%E3%83%9F%E3%83%B3%E3%82%B0PC%20RTX4070%20%E3%83%A1%E3%83%A2%E3%83%AA32GB&tag=negi3939-22)** - ローカルLLM実行に最適なスペック


<div style="display: flex; gap: 10px; flex-wrap: wrap; margin: 15px 0;">
<a href="https://www.amazon.co.jp/s?k=%E4%B8%AD%E5%8F%A4%20PC%20%E7%AC%AC8%E4%B8%96%E4%BB%A3%20Core%20i5%2016GB&tag=negi3939-22" target="_blank" rel="noopener sponsored" style="padding: 10px 20px; background: linear-gradient(135deg, #ff9900, #ff6600); color: white; text-decoration: none; border-radius: 6px; font-weight: bold;">🔍 Amazonで「中古 PC 第8世代 Core i5 16GB」を検索</a>
<a href="https://hb.afl.rakuten.co.jp/hgc/5000cbfd.5f52567b.5000cbff.924460a4/?pc=https%3A%2F%2Fsearch.rakuten.co.jp%2Fsearch%2Fmall%2F%25E4%25B8%25AD%25E5%258F%25A4%2520PC%2520%25E7%25AC%25AC8%25E4%25B8%2596%25E4%25BB%25A3%2520Core%2520i5%252016GB%2F&m=https%3A%2F%2Fsearch.rakuten.co.jp%2Fsearch%2Fmall%2F%25E4%25B8%25AD%25E5%258F%25A4%2520PC%2520%25E7%25AC%25AC8%25E4%25B8%2596%25E4%25BB%25A3%2520Core%2520i5%252016GB%2F" target="_blank" rel="noopener sponsored" style="padding: 10px 20px; background: linear-gradient(135deg, #bf0000, #8b0000); color: white; text-decoration: none; border-radius: 6px; font-weight: bold;">🔍 楽天で検索</a>
</div>

<small style="color: #888;">※上記リンクはアフィリエイトリンクです。</small>
